{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline ML Modulare per Cybersecurity: Guida Completa\n",
    "\n",
    "Questo notebook è una guida interattiva a una pipeline di machine learning end-to-end per la classificazione di traffico di rete. È stato progettato con tre principi chiave in mente:\n",
    "\n",
    "- **Modularità**: Ogni fase logica (preprocessing, training, predizione) è isolata nel proprio modulo Python (`.py`), seguendo il Single Responsibility Principle. Questo rende il codice più pulito, facile da mantenere e da testare.\n",
    "- **Configurabilità**: Tutta la pipeline è controllata da un singolo file, `config.py`. Questo agisce come un \"pannello di controllo\" per cambiare dataset, selezionare feature e fare tuning degli iperparametri senza toccare il codice della logica.\n",
    "- **Estensibilità**: La struttura è pensata per essere facilmente adattabile a nuovi dataset o a nuove architetture di modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architettura e Scelte Tecniche\n",
    "\n",
    "La pipeline è composta dai seguenti elementi:\n",
    "\n",
    "- `create_synthetic_data.py`: Uno script per generare dati di esempio realistici. Utile per testare la pipeline senza bisogno di un dataset reale.\n",
    "- `config.py`: Il cuore della configurazione. Qui si definiscono i percorsi, le colonne da usare e gli iperparametri per il training.\n",
    "- `preprocessing/process.py`: Contiene tutta la logica per caricare i dati e trasformarli in un formato numerico che il modello può comprendere (es. anonimizzazione IP, one-hot encoding).\n",
    "- `training/train.py`: Gestisce la creazione del modello, la grid search per trovare i migliori iperparametri e il salvataggio del modello più performante.\n",
    "- `prediction/predict.py`: Carica un modello salvato e lo usa per fare predizioni su nuovi dati, applicando la stessa logica di preprocessing.\n",
    "\n",
    "**Librerie Utilizzate:**\n",
    "- **Pandas**: Per la manipolazione efficiente dei dati tabulari.\n",
    "- **Scikit-learn**: Per utility di preprocessing standard come `LabelEncoder` e per la divisione dei dati in set di training e test.\n",
    "- **TensorFlow (con Keras)**: Come framework di deep learning. È stato scelto per la sua flessibilità e facilità d'uso nel costruire modelli di rete neurale.\n",
    "- **Faker**: Per generare dati fittizi ma realistici (come gli indirizzi IP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Iniziale\n",
    "\n",
    "Installiamo le librerie necessarie per eseguire l'intera pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa le dipendenze\n",
    "!pip install pandas scikit-learn tensorflow faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Esecuzione della Pipeline\n",
    "\n",
    "Ora eseguiamo la pipeline passo dopo passo, importando le funzioni dai nostri moduli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 1: Generazione del Dataset Sintetico\n",
    "\n",
    "Creiamo un dataset di esempio per poter lavorare. Lo script salverà il file in `data/cybersecurity_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_synthetic_data import generate_synthetic_data\n",
    "generate_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 2: Preprocessing dei Dati\n",
    "\n",
    "Questa è una fase cruciale. Il modulo `process.py` esegue diverse trasformazioni chiave:\n",
    "\n",
    "- **Anonimizzazione IP**: I modelli di machine learning non possono processare stringhe come '192.168.1.1'. Questa funzione mappa ogni indirizzo IP univoco a un ID numerico (es. 1, 2, 3...). La mappa (`ip -> id` e `id -> ip`) viene salvata per poterla riutilizzare durante la predizione.\n",
    "- **One-Hot Encoding**: La colonna `protocollo` contiene valori come 'TCP', 'UDP'. Per evitare che il modello interpreti una relazione numerica errata (es. TCP=0, UDP=1, ICMP=2), trasformiamo questa colonna in più colonne binarie (`protocollo_TCP`, `protocollo_UDP`), dove solo una può essere 1 (vera).\n",
    "- **Encoding del Target**: Similmente, le etichette di attacco ('normal', 'dos', ecc.) vengono mappate in interi, necessari per la funzione di loss del modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.process import preprocess_data\n",
    "\n",
    "X, y, _, _ = preprocess_data()\n",
    "\n",
    "if X is not None:\n",
    "    print(\"\\n--- Anteprima Feature (X) Processate ---\")\n",
    "    display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 3: Training e Tuning degli Iperparametri\n",
    "\n",
    "Ora addestriamo il modello. Il modulo `train.py` esegue una **Grid Search**: testa diverse combinazioni di iperparametri definite in `config.py` per trovare la configurazione migliore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.train import train_and_evaluate\n",
    "\n",
    "_, _ = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guida alla Configurazione degli Iperparametri (`config.py`)\n",
    "\n",
    "La grid search testa le combinazioni dei valori che inserisci nelle liste in `TRAINING_CONFIG['hyperparameters']`. Ecco una guida su come sceglierli:\n",
    "\n",
    "- `activation` (`'relu'`, `'tanh'`): Funzione di attivazione dei neuroni. `'relu'` è un'ottima scelta di default, veloce ed efficiente. `'tanh'` può essere utile in reti più profonde ma è più costosa computazionalmente.\n",
    "\n",
    "- `batch_size` (`16`, `32`): Numero di campioni processati prima di aggiornare il modello. \n",
    "  - **Valori Bassi (es. 16, 32)**: L'addestramento è più lento ma può convergere meglio (generalizzare) perché gli aggiornamenti sono più frequenti e \"rumorosi\". Ideale per dataset complessi e non enormi.\n",
    "  - **Valori Alti (es. 128, 256)**: L'addestramento è molto più veloce. Utile per dataset molto grandi e stabili, ma c'è il rischio di convergere a soluzioni non ottimali.\n",
    "\n",
    "- `epochs` (`50`): Numero di volte in cui il modello vedrà l'intero dataset. Un numero troppo basso porta a *underfitting* (il modello non impara abbastanza), un numero troppo alto a *overfitting* (il modello impara a memoria i dati di training e non generalizza). Il valore giusto si trova sperimentalmente, monitorando la performance sul set di validazione.\n",
    "\n",
    "- `learning_rate` (`0.001`, `0.01`): Controlla la dimensione dei \"passi\" durante l'ottimizzazione. \n",
    "  - **Valore Basso (es. 0.0001)**: Passi piccoli, training lento ma più probabilità di trovare un buon minimo. \n",
    "  - **Valore Alto (es. 0.01)**: Passi grandi, training veloce ma si rischia di \"saltare\" la soluzione ottimale.\n",
    "\n",
    "- `hidden_layer_size` (`32`, `64`): Numero di neuroni nel layer nascosto. Controlla la **capacità** del modello. Più neuroni possono apprendere pattern più complessi, ma aumentano il rischio di overfitting e il costo computazionale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fase 4: Predizione su un Nuovo Campione\n",
    "\n",
    "Infine, usiamo il modello migliore salvato per fare una predizione su un nuovo dato, che non ha mai visto prima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.predict import predict\n",
    "\n",
    "# Creiamo un campione di dati fittizio\n",
    "sample = {\n",
    "    \"ip_sorgente\": \"192.168.1.10\",\n",
    "    \"ip_destinazione\": \"10.0.0.5\",\n",
    "    \"porta_sorgente\": 12345,\n",
    "    \"porta_destinazione\": 443,\n",
    "    \"protocollo\": \"UDP\",\n",
    "    \"byte_inviati\": 250,\n",
    "    \"byte_ricevuti\": 1800,\n",
    "    \"pacchetti_inviati\": 8,\n",
    "    \"pacchetti_ricevuti\": 12,\n",
    "}\n",
    "\n",
    "prediction_label = predict(sample)\n",
    "\n",
    "if prediction_label:\n",
    "    print(f\"\\nRISULTATO FINALE: Il campione è stato classificato come: '{prediction_label}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Come Adattare la Pipeline al Tuo Dataset\n",
    "\n",
    "La forza di questa architettura è la facilità con cui puoi usarla per i tuoi dati. Ecco come fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passaggio 1: Aggiorna `config.py`\n",
    "\n",
    "Immagina di avere un nuovo dataset `my_traffic.csv` con le seguenti colonne: `['Timestamp', 'SourceIP', 'DestIP', 'SourcePort', 'DestPort', 'ProtocolType', 'PacketCount', 'TrafficType']`.\n",
    "\n",
    "Per adattare la pipeline, dovrai modificare `DATA_CONFIG` in `config.py` così:\n",
    "\n",
    "```python\n",
    "DATA_CONFIG = {\n",
    "    # 1. Cambia il percorso del file\n",
    "    \"dataset_path\": \"data/my_traffic.csv\",\n",
    "\n",
    "    # 2. Definisci le tue feature numeriche\n",
    "    \"numeric_feature_columns\": [\n",
    "        \"SourcePort\",\n",
    "        \"DestPort\",\n",
    "        \"PacketCount\"\n",
    "    ],\n",
    "    \n",
    "    # 3. Definisci le tue feature categoriche\n",
    "    \"one_hot_encode_columns\": [\"ProtocolType\"],\n",
    "\n",
    "    # 4. Definisci le tue colonne IP\n",
    "    \"ip_columns_to_anonymize\": [\"SourceIP\", \"DestIP\"],\n",
    "\n",
    "    # 5. Definisci la tua colonna target\n",
    "    \"target_column\": \"TrafficType\",\n",
    "\n",
    "    \"anonymize_target\": True,\n",
    "}\n",
    "```\n",
    "**Fatto!** Eseguendo di nuovo il notebook o gli script, la pipeline userà automaticamente il tuo nuovo dataset e le tue colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passaggio 2: Creare Nuove Feature (Feature Engineering)\n",
    "\n",
    "Spesso, le feature migliori non sono quelle originali, ma quelle che creiamo noi. Immaginiamo di voler creare una feature `is_well_known_port` che è `1` se la porta di destinazione è una porta comune (es. 80, 443) e `0` altrimenti.\n",
    "\n",
    "Puoi farlo modificando leggermente `preprocessing/process.py`:\n",
    "\n",
    "1. Apri `preprocessing/process.py`.\n",
    "2. Trova la sezione dove viene caricato il DataFrame `df`.\n",
    "3. **Aggiungi il tuo codice** per creare la nuova colonna, prima che le feature vengano assemblate in `X`.\n",
    "\n",
    "```python\n",
    "# Esempio da aggiungere in process.py dopo il caricamento di df\n",
    "\n",
    "well_known_ports = [80, 443, 22, 21, 53]\n",
    "df['is_well_known_port'] = df['porta_destinazione'].isin(well_known_ports).astype(int)\n",
    "```\n",
    "\n",
    "4. Infine, **aggiorna `config.py`** per includere la tua nuova feature nella lista delle colonne numeriche:\n",
    "\n",
    "```python\n",
    "\"numeric_feature_columns\": [\n",
    "    \"porta_sorgente\",\n",
    "    \"porta_destinazione\",\n",
    "    # ... altre colonne ...\n",
    "    \"is_well_known_port\" # <-- Aggiungi qui la tua nuova feature\n",
    "],\n",
    "```\n",
    "Questo approccio ti dà la flessibilità di creare logiche di preprocessing complesse mantenendo la configurazione semplice e centralizzata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
