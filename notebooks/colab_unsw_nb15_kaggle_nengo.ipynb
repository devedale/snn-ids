{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clona la repo e installa requisiti compatibili con NengoDL\n",
        "!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\n",
        "%cd /content/snn-ids\n",
        "\n",
        "# Disinstalla eventuali Keras 3 e pacchetti confliggenti\n",
        "!pip -q uninstall -y keras keras-nightly tf-keras keras-preprocessing keras-vis || true\n",
        "\n",
        "# Installa versioni testate con NengoDL + kaggle\n",
        "!pip -q install \"tensorflow==2.13.1\" \"keras==2.13.1\" \"nengo==3.2.0\" \"nengo-dl==3.6.0\" kaggle pandas scikit-learn pyyaml\n",
        "\n",
        "import tensorflow as tf, keras\n",
        "print(\"TF:\", tf.__version__, \"Keras:\", keras.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fissa numpy/typing_extensions e riavvia runtime per allineare ABI\n",
        "!pip -q install \"numpy==1.23.5\" \"typing_extensions==4.8.0\" \"protobuf<6.0.0\"\n",
        "import os\n",
        "print(\"Riavvio runtime per completare l'installazione...\")\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN (Nengo/NengoDL) su UNSW-NB15 — Colab via Kaggle\n",
        "\n",
        "- Scarica UNSW-NB15 con Kaggle API (`kaggle.json`)\n",
        "- Preprocessing minimale\n",
        "- Rete Nengo LIF + training con NengoDL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Kaggle\n",
        "import os, json\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# carica il tuo kaggle.json in /content/kaggle.json prima di eseguire\n",
        "if os.path.exists('/content/kaggle.json'):\n",
        "  !cp -f /content/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json || true\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download UNSW-NB15 da Kaggle (aggiorna lo slug se necessario)\n",
        "from pathlib import Path\n",
        "Path('/content/data').mkdir(exist_ok=True)\n",
        "!kaggle datasets download -d mOhammad/unsw-nb15 -p /content/data -q\n",
        "!unzip -q -o /content/data/*.zip -d /content/data\n",
        "!ls -lh /content/data | head -n 20\n",
        "\n",
        "# Esempio: conversione CSV→dataset SNN (sostituisci il path esatto del CSV)\n",
        "# !python -m tools.csv_to_snn_dataset --input /content/data/UNSW_NB15_training-set.csv --output /content/data/unsw_snn.csv --label-col label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing minimo (adatta a seconda dei CSV)\n",
        "import pandas as pd, numpy as np\n",
        "from glob import glob\n",
        "files = sorted(glob('/content/data/**/*.csv', recursive=True)) or sorted(glob('/content/data/*.csv'))\n",
        "print(len(files), 'csv trovati')\n",
        "\n",
        "dfs = []\n",
        "for f in files[:5]:\n",
        "  try:\n",
        "    df = pd.read_csv(f)\n",
        "    dfs.append(df)\n",
        "  except Exception as e:\n",
        "    print('skip', f, e)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(df.shape)\n",
        "\n",
        "label_col = 'label' if 'label' in df.columns else ('Label' if 'Label' in df.columns else None)\n",
        "assert label_col is not None, 'Colonna etichetta non trovata'\n",
        "labels = df[label_col].astype(str).str.lower()\n",
        "y = (~labels.str.contains('normal') & ~labels.str.contains('benign')).astype(int).values\n",
        "\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_candidates = [c for c in df.columns if df[c].dtype == object and c != label_col]\n",
        "cat_cols = cat_candidates[:3]\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_num = scaler.fit_transform(df[num_cols].fillna(0).values.astype(float)) if num_cols else np.zeros((len(df),0))\n",
        "X_cat = pd.get_dummies(df[cat_cols].fillna('NA').astype(str)).values if cat_cols else np.zeros((len(df),0))\n",
        "X = np.hstack([X_num, X_cat]).astype(np.float32)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "input_size, output_size = X_train.shape[1], 2\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Nengo: rete LIF e training con NengoDL\n",
        "import nengo, nengo_dl, tensorflow as tf\n",
        "\n",
        "timesteps = 10\n",
        "with nengo.Network() as net:\n",
        "  inp = nengo.Node(np.zeros(input_size))\n",
        "  ens1 = nengo.Ensemble(n_neurons=256, dimensions=input_size, neuron_type=nengo.LIF())\n",
        "  nengo.Connection(inp, ens1)\n",
        "  ens2 = nengo.Ensemble(n_neurons=output_size, dimensions=output_size, neuron_type=nengo.LIF())\n",
        "  nengo.Connection(ens1, ens2)\n",
        "  p_out = nengo.Probe(ens2, synapse=0.1)\n",
        "\n",
        "X_train_enc = np.repeat(X_train[:, None, :], timesteps, axis=1)\n",
        "X_test_enc  = np.repeat(X_test[:,  None, :], timesteps, axis=1)\n",
        "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=output_size)\n",
        "y_test_oh  = tf.keras.utils.to_categorical(y_test,  num_classes=output_size)\n",
        "y_train_enc = np.repeat(y_train_oh[:, None, :], timesteps, axis=1)\n",
        "y_test_enc  = np.repeat(y_test_oh[:,  None, :], timesteps, axis=1)\n",
        "\n",
        "minibatch = 128\n",
        "with nengo_dl.Simulator(net, minibatch_size=minibatch) as sim:\n",
        "  inputs = {inp: X_train_enc}\n",
        "  targets = {p_out: y_train_enc}\n",
        "  sim.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.MSE)\n",
        "  sim.fit(inputs, targets, epochs=5, validation_split=0.2, verbose=2)\n",
        "  test_dict = {inp: X_test_enc}\n",
        "  sim.run_steps(X_test_enc.shape[1], data=test_dict)\n",
        "  out = sim.data[p_out]\n",
        "\n",
        "pred = out.mean(axis=1)\n",
        "acc = (pred.argmax(axis=1) == y_test).mean()\n",
        "print('Test accuracy:', acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvataggio rete Nengo\n",
        "import pickle, os\n",
        "os.makedirs('/content/out', exist_ok=True)\n",
        "with open('/content/out/nengo_unsw_nb15.pkl', 'wb') as f:\n",
        "  pickle.dump(net, f)\n",
        "print('Salvato /content/out/nengo_unsw_nb15.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
