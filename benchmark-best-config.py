#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Benchmark Configurazione Migliore SNN-IDS
Test e fine-tuning della configurazione ottimale identificata:
GRU - epochs=15, batch=64, lr=0.01, tanh, units=64
Accuracy: 96.24%
"""

import os
import sys
import json
import time
import argparse
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any

# Import del benchmark esistente
sys.path.append(os.path.abspath('.'))
from benchmark import SNNIDSBenchmark
from config import TRAINING_CONFIG, PREPROCESSING_CONFIG, DATA_CONFIG

class BestConfigBenchmark:
    """Benchmark dedicato alla configurazione migliore con fine-tuning."""
    
    def __init__(self, sample_size: int = None, data_path: str = None):
        """Inizializza il benchmark per la configurazione migliore."""
        self.sample_size = sample_size or PREPROCESSING_CONFIG['sample_size']
        self.data_path = data_path or DATA_CONFIG['dataset_path']
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f"best_config_benchmark_{self.timestamp}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Configurazione base migliore identificata
        self.best_base_config = {
            'model_type': 'gru',
            'epochs': 15,
            'batch_size': 64,
            'learning_rate': 0.01,
            'activation': 'tanh',
            'gru_units': 64
        }
        
        print(f"ğŸ† BENCHMARK CONFIGURAZIONE MIGLIORE")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        print(f"ğŸ“Š Sample size: {self.sample_size}")
        print(f"âš™ï¸  Configurazione base: GRU, epochs=15, batch=64, lr=0.01, tanh, units=64")
    
    def test_baseline_config(self) -> Dict:
        """Testa la configurazione base per confermare le performance."""
        print(f"\nğŸ¯ TEST BASELINE - Conferma Configurazione Migliore")
        print(f"{'â”€'*60}")
        
        hyperparams = {
            'epochs': [self.best_base_config['epochs']],
            'batch_size': [self.best_base_config['batch_size']],
            'learning_rate': [self.best_base_config['learning_rate']],
            'activation': [self.best_base_config['activation']],
            'gru_units': [self.best_base_config['gru_units']]
        }
        
        result = self._run_single_test('gru', hyperparams, 'baseline_confirmation')
        
        if result.get('status') == 'success':
            accuracy = result['best_accuracy']
            print(f"âœ… Baseline confermato: {accuracy:.6f}")
            return result
        else:
            print(f"âŒ Errore nel test baseline: {result.get('error', 'Unknown')}")
            return result
    
    def fine_tune_learning_rate(self) -> Dict:
        """Fine-tuning granulare del learning rate attorno a 0.01."""
        print(f"\nğŸ” FINE-TUNING LEARNING RATE")
        print(f"ğŸ“Œ Range: 0.008 - 0.015 attorno al valore ottimale 0.01")
        print(f"{'â”€'*60}")
        
        learning_rates = [0.008, 0.009, 0.01, 0.011, 0.012, 0.015]
        results = []
        
        for lr in learning_rates:
            print(f"  ğŸ§ª Testing learning_rate = {lr}")
            
            hyperparams = {
                'epochs': [self.best_base_config['epochs']],
                'batch_size': [self.best_base_config['batch_size']],
                'learning_rate': [lr],
                'activation': [self.best_base_config['activation']],
                'gru_units': [self.best_base_config['gru_units']]
            }
            
            result = self._run_single_test('gru', hyperparams, f'lr_tuning_{lr}')
            results.append(result)
            
            if result.get('status') == 'success':
                accuracy = result['best_accuracy']
                print(f"    âœ… Accuracy: {accuracy:.6f}")
            else:
                print(f"    âŒ Errore: {result.get('error', 'Unknown')}")
        
        # Trova il migliore
        best_result = max([r for r in results if r.get('status') == 'success'], 
                         key=lambda x: x.get('best_accuracy', 0), default=None)
        
        if best_result:
            best_lr = best_result['config']['hyperparameters']['learning_rate'][0]
            best_acc = best_result['best_accuracy']
            print(f"\nğŸ† Miglior learning rate: {best_lr} (accuracy: {best_acc:.6f})")
            self.best_base_config['learning_rate'] = best_lr
        
        return {'best_result': best_result, 'all_results': results}
    
    def fine_tune_gru_units(self) -> Dict:
        """Fine-tuning delle unitÃ  GRU attorno a 64."""
        print(f"\nğŸ§  FINE-TUNING GRU UNITS")
        print(f"ğŸ“Œ Range: 56-80 attorno al valore ottimale 64")
        print(f"{'â”€'*60}")
        
        gru_units = [56, 60, 64, 68, 72, 80]
        results = []
        
        for units in gru_units:
            print(f"  ğŸ§ª Testing gru_units = {units}")
            
            hyperparams = {
                'epochs': [self.best_base_config['epochs']],
                'batch_size': [self.best_base_config['batch_size']],
                'learning_rate': [self.best_base_config['learning_rate']],
                'activation': [self.best_base_config['activation']],
                'gru_units': [units]
            }
            
            result = self._run_single_test('gru', hyperparams, f'units_tuning_{units}')
            results.append(result)
            
            if result.get('status') == 'success':
                accuracy = result['best_accuracy']
                print(f"    âœ… Accuracy: {accuracy:.6f}")
            else:
                print(f"    âŒ Errore: {result.get('error', 'Unknown')}")
        
        # Trova il migliore
        best_result = max([r for r in results if r.get('status') == 'success'], 
                         key=lambda x: x.get('best_accuracy', 0), default=None)
        
        if best_result:
            best_units = best_result['config']['hyperparameters']['gru_units'][0]
            best_acc = best_result['best_accuracy']
            print(f"\nğŸ† Miglior GRU units: {best_units} (accuracy: {best_acc:.6f})")
            self.best_base_config['gru_units'] = best_units
        
        return {'best_result': best_result, 'all_results': results}
    
    def fine_tune_epochs(self) -> Dict:
        """Fine-tuning del numero di epoche attorno a 15."""
        print(f"\nğŸ“ˆ FINE-TUNING EPOCHS")
        print(f"ğŸ“Œ Range: 12-20 attorno al valore ottimale 15")
        print(f"{'â”€'*60}")
        
        epochs_list = [12, 13, 15, 17, 18, 20]
        results = []
        
        for epochs in epochs_list:
            print(f"  ğŸ§ª Testing epochs = {epochs}")
            
            hyperparams = {
                'epochs': [epochs],
                'batch_size': [self.best_base_config['batch_size']],
                'learning_rate': [self.best_base_config['learning_rate']],
                'activation': [self.best_base_config['activation']],
                'gru_units': [self.best_base_config['gru_units']]
            }
            
            result = self._run_single_test('gru', hyperparams, f'epochs_tuning_{epochs}')
            results.append(result)
            
            if result.get('status') == 'success':
                accuracy = result['best_accuracy']
                training_time = result.get('training_time', 0)
                print(f"    âœ… Accuracy: {accuracy:.6f} (tempo: {training_time:.1f}s)")
            else:
                print(f"    âŒ Errore: {result.get('error', 'Unknown')}")
        
        # Trova il migliore
        best_result = max([r for r in results if r.get('status') == 'success'], 
                         key=lambda x: x.get('best_accuracy', 0), default=None)
        
        if best_result:
            best_epochs = best_result['config']['hyperparameters']['epochs'][0]
            best_acc = best_result['best_accuracy']
            print(f"\nğŸ† Miglior epochs: {best_epochs} (accuracy: {best_acc:.6f})")
            self.best_base_config['epochs'] = best_epochs
        
        return {'best_result': best_result, 'all_results': results}
    
    def fine_tune_batch_size(self) -> Dict:
        """Fine-tuning della batch size attorno a 64."""
        print(f"\nğŸ“¦ FINE-TUNING BATCH SIZE")
        print(f"ğŸ“Œ Range: 48-96 attorno al valore ottimale 64")
        print(f"{'â”€'*60}")
        
        batch_sizes = [48, 56, 64, 72, 80, 96]
        results = []
        
        for batch_size in batch_sizes:
            print(f"  ğŸ§ª Testing batch_size = {batch_size}")
            
            hyperparams = {
                'epochs': [self.best_base_config['epochs']],
                'batch_size': [batch_size],
                'learning_rate': [self.best_base_config['learning_rate']],
                'activation': [self.best_base_config['activation']],
                'gru_units': [self.best_base_config['gru_units']]
            }
            
            result = self._run_single_test('gru', hyperparams, f'batch_tuning_{batch_size}')
            results.append(result)
            
            if result.get('status') == 'success':
                accuracy = result['best_accuracy']
                training_time = result.get('training_time', 0)
                print(f"    âœ… Accuracy: {accuracy:.6f} (tempo: {training_time:.1f}s)")
            else:
                print(f"    âŒ Errore: {result.get('error', 'Unknown')}")
        
        # Trova il migliore
        best_result = max([r for r in results if r.get('status') == 'success'], 
                         key=lambda x: x.get('best_accuracy', 0), default=None)
        
        if best_result:
            best_batch = best_result['config']['hyperparameters']['batch_size'][0]
            best_acc = best_result['best_accuracy']
            print(f"\nğŸ† Miglior batch size: {best_batch} (accuracy: {best_acc:.6f})")
            self.best_base_config['batch_size'] = best_batch
        
        return {'best_result': best_result, 'all_results': results}
    
    def test_regularization(self) -> Dict:
        """Testa l'aggiunta di dropout per prevenire overfitting."""
        print(f"\nğŸ›¡ï¸ TEST REGULARIZATION (Dropout)")
        print(f"ğŸ“Œ Valori: 0.0, 0.1, 0.2, 0.3")
        print(f"{'â”€'*60}")
        
        dropout_values = [0.0, 0.1, 0.2, 0.3]
        results = []
        
        for dropout in dropout_values:
            print(f"  ğŸ§ª Testing dropout = {dropout}")
            
            hyperparams = {
                'epochs': [self.best_base_config['epochs']],
                'batch_size': [self.best_base_config['batch_size']],
                'learning_rate': [self.best_base_config['learning_rate']],
                'activation': [self.best_base_config['activation']],
                'gru_units': [self.best_base_config['gru_units']],
                'dropout': [dropout]
            }
            
            result = self._run_single_test('gru', hyperparams, f'dropout_tuning_{dropout}')
            results.append(result)
            
            if result.get('status') == 'success':
                accuracy = result['best_accuracy']
                print(f"    âœ… Accuracy: {accuracy:.6f}")
            else:
                print(f"    âŒ Errore: {result.get('error', 'Unknown')}")
        
        # Trova il migliore
        best_result = max([r for r in results if r.get('status') == 'success'], 
                         key=lambda x: x.get('best_accuracy', 0), default=None)
        
        if best_result:
            best_dropout = best_result['config']['hyperparameters']['dropout'][0]
            best_acc = best_result['best_accuracy']
            print(f"\nğŸ† Miglior dropout: {best_dropout} (accuracy: {best_acc:.6f})")
            if best_dropout > 0:
                self.best_base_config['dropout'] = best_dropout
        
        return {'best_result': best_result, 'all_results': results}
    
    def run_complete_fine_tuning(self) -> Dict:
        """Esegue il fine-tuning completo della configurazione migliore."""
        print(f"\n{'='*70}")
        print(f"ğŸ¯ FINE-TUNING COMPLETO CONFIGURAZIONE MIGLIORE")
        print(f"{'='*70}")
        
        start_time = time.time()
        all_phases = {}
        
        # Fase 1: Test baseline
        print(f"\nğŸ“‹ FASE 1/5: Conferma Baseline")
        baseline_result = self.test_baseline_config()
        all_phases['baseline'] = baseline_result
        baseline_accuracy = baseline_result.get('best_accuracy', 0) if baseline_result.get('status') == 'success' else 0
        
        # Fase 2: Fine-tuning learning rate
        print(f"\nğŸ“‹ FASE 2/5: Fine-tuning Learning Rate")
        lr_results = self.fine_tune_learning_rate()
        all_phases['learning_rate'] = lr_results
        
        # Fase 3: Fine-tuning GRU units
        print(f"\nğŸ“‹ FASE 3/5: Fine-tuning GRU Units")
        units_results = self.fine_tune_gru_units()
        all_phases['gru_units'] = units_results
        
        # Fase 4: Fine-tuning epochs
        print(f"\nğŸ“‹ FASE 4/5: Fine-tuning Epochs")
        epochs_results = self.fine_tune_epochs()
        all_phases['epochs'] = epochs_results
        
        # Fase 5: Fine-tuning batch size
        print(f"\nğŸ“‹ FASE 5/5: Fine-tuning Batch Size")
        batch_results = self.fine_tune_batch_size()
        all_phases['batch_size'] = batch_results
        
        # Fase 6: Test regularization
        print(f"\nğŸ“‹ FASE BONUS: Test Regularization")
        dropout_results = self.test_regularization()
        all_phases['regularization'] = dropout_results
        
        # Test finale con configurazione ottimizzata
        print(f"\nğŸ TEST FINALE - Configurazione Ottimizzata")
        final_result = self.test_final_optimized_config()
        all_phases['final_optimized'] = final_result
        
        total_time = time.time() - start_time
        
        # Genera rapporto
        report = self._generate_fine_tuning_report(all_phases, baseline_accuracy, total_time)
        
        # Salva risultati
        self._save_results(report)
        
        print(f"\n{'='*70}")
        print(f"ğŸ‰ FINE-TUNING COMPLETATO!")
        print(f"â±ï¸  Tempo totale: {total_time:.2f}s")
        print(f"ğŸ“ˆ Miglioramento: {report.get('improvement', {}).get('absolute', 0):.6f}")
        print(f"ğŸ† Accuracy finale: {report.get('final_accuracy', 0):.6f}")
        print(f"ğŸ“ Risultati salvati in: {self.output_dir}")
        print(f"{'='*70}")
        
        return report
    
    def test_final_optimized_config(self) -> Dict:
        """Test finale con la configurazione completamente ottimizzata."""
        print(f"ğŸ Configurazione finale ottimizzata:")
        for key, value in self.best_base_config.items():
            print(f"    {key}: {value}")
        
        hyperparams = {
            'epochs': [self.best_base_config['epochs']],
            'batch_size': [self.best_base_config['batch_size']],
            'learning_rate': [self.best_base_config['learning_rate']],
            'activation': [self.best_base_config['activation']],
            'gru_units': [self.best_base_config['gru_units']]
        }
        
        if 'dropout' in self.best_base_config:
            hyperparams['dropout'] = [self.best_base_config['dropout']]
        
        result = self._run_single_test('gru', hyperparams, 'final_optimized')
        
        if result.get('status') == 'success':
            accuracy = result['best_accuracy']
            print(f"âœ… Accuracy finale: {accuracy:.6f}")
        
        return result
    
    def _run_single_test(self, model_type: str, hyperparams: Dict, test_id: str) -> Dict:
        """Esegue un singolo test usando SNNIDSBenchmark._run_single_configuration direttamente."""
        config_override = {
            'sample_size': self.sample_size,
            'data_path': self.data_path
        }
        
        benchmark = SNNIDSBenchmark(config_override)
        
        # Configurazione specifica per il singolo modello
        test_config = {
            'sample_size': self.sample_size,
            'data_path': self.data_path,
            'model_type': model_type,  # Solo il modello specificato
            'hyperparameters': hyperparams
        }
        
        # Usa direttamente _run_single_configuration per evitare il loop su tutti i modelli
        result = benchmark._run_single_configuration(test_config)
        result['test_id'] = test_id
        
        return result
    
    def _generate_fine_tuning_report(self, all_phases: Dict, baseline_accuracy: float, total_time: float) -> Dict:
        """Genera il rapporto completo del fine-tuning."""
        
        # Trova la migliore accuracy finale
        final_result = all_phases.get('final_optimized', {})
        final_accuracy = final_result.get('best_accuracy', 0) if final_result.get('status') == 'success' else 0
        
        # Calcola miglioramento
        improvement = final_accuracy - baseline_accuracy
        improvement_percent = (improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else 0
        
        # Statistiche per fase
        phase_stats = {}
        for phase_name, phase_data in all_phases.items():
            if phase_name == 'final_optimized':
                continue
                
            if isinstance(phase_data, dict) and 'all_results' in phase_data:
                successful_tests = [r for r in phase_data['all_results'] if r.get('status') == 'success']
                best_in_phase = max(successful_tests, key=lambda x: x.get('best_accuracy', 0), default=None)
                
                phase_stats[phase_name] = {
                    'tests_run': len(phase_data['all_results']),
                    'successful_tests': len(successful_tests),
                    'best_accuracy': best_in_phase.get('best_accuracy', 0) if best_in_phase else 0
                }
            elif isinstance(phase_data, dict) and 'best_accuracy' in phase_data:
                # Baseline case
                phase_stats[phase_name] = {
                    'tests_run': 1,
                    'successful_tests': 1 if phase_data.get('status') == 'success' else 0,
                    'best_accuracy': phase_data.get('best_accuracy', 0)
                }
        
        report = {
            'timestamp': self.timestamp,
            'total_time': total_time,
            'sample_size': self.sample_size,
            'baseline_accuracy': baseline_accuracy,
            'final_accuracy': final_accuracy,
            'improvement': {
                'absolute': improvement,
                'percent': improvement_percent
            },
            'optimized_config': self.best_base_config.copy(),
            'phase_statistics': phase_stats,
            'detailed_results': all_phases
        }
        
        return report
    
    def _save_results(self, report: Dict):
        """Salva tutti i risultati del fine-tuning."""
        
        # JSON completo
        json_file = os.path.join(self.output_dir, "fine_tuning_complete.json")
        with open(json_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        print(f"ğŸ’¾ Rapporto completo: {json_file}")
        
        # Summary JSON
        summary = {
            'timestamp': report['timestamp'],
            'baseline_accuracy': report['baseline_accuracy'],
            'final_accuracy': report['final_accuracy'],
            'improvement': report['improvement'],
            'optimized_config': report['optimized_config'],
            'total_time': report['total_time']
        }
        summary_file = os.path.join(self.output_dir, "fine_tuning_summary.json")
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        print(f"ğŸ“‹ Summary: {summary_file}")
        
        # CSV per analisi
        self._save_csv_analysis(report)
        
        # Report testuale
        self._save_text_report(report)
    
    def _save_csv_analysis(self, report: Dict):
        """Salva analisi CSV dei risultati."""
        try:
            all_tests = []
            
            for phase_name, phase_data in report['detailed_results'].items():
                if phase_name == 'final_optimized':
                    continue
                    
                if isinstance(phase_data, dict) and 'all_results' in phase_data:
                    for result in phase_data['all_results']:
                        if result.get('status') != 'success':
                            continue
                        
                        config = result.get('config', {})
                        hyperparams = config.get('hyperparameters', {})
                        
                        test_row = {
                            'phase': phase_name,
                            'test_id': result.get('test_id', 'unknown'),
                            'accuracy': result.get('best_accuracy'),
                            'training_time': result.get('training_time'),
                            'epochs': hyperparams.get('epochs', [None])[0],
                            'batch_size': hyperparams.get('batch_size', [None])[0],
                            'learning_rate': hyperparams.get('learning_rate', [None])[0],
                            'gru_units': hyperparams.get('gru_units', [None])[0],
                            'dropout': hyperparams.get('dropout', [None])[0] if 'dropout' in hyperparams else None
                        }
                        all_tests.append(test_row)
            
            if all_tests:
                df = pd.DataFrame(all_tests)
                csv_file = os.path.join(self.output_dir, "fine_tuning_analysis.csv")
                df.to_csv(csv_file, index=False)
                print(f"ğŸ“Š Analisi CSV: {csv_file}")
                
        except ImportError:
            print("âš ï¸ Pandas non disponibile, CSV analysis saltata")
        except Exception as e:
            print(f"âŒ Errore nel salvataggio CSV: {e}")
    
    def _save_text_report(self, report: Dict):
        """Salva rapporto testuale leggibile."""
        report_file = os.path.join(self.output_dir, "fine_tuning_report.txt")
        
        with open(report_file, 'w') as f:
            f.write("ğŸ† RAPPORTO FINE-TUNING CONFIGURAZIONE MIGLIORE\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"ğŸ“… Timestamp: {report['timestamp']}\n")
            f.write(f"â±ï¸  Tempo totale: {report['total_time']:.2f}s\n")
            f.write(f"ğŸ“Š Sample size: {report['sample_size']}\n\n")
            
            f.write("ğŸ“ˆ RISULTATI:\n")
            f.write("-" * 30 + "\n")
            f.write(f"ğŸ¯ Accuracy baseline: {report['baseline_accuracy']:.6f}\n")
            f.write(f"ğŸ† Accuracy finale: {report['final_accuracy']:.6f}\n")
            f.write(f"ğŸ“ˆ Miglioramento: +{report['improvement']['absolute']:.6f} ({report['improvement']['percent']:.2f}%)\n\n")
            
            f.write("âš™ï¸  CONFIGURAZIONE OTTIMIZZATA:\n")
            f.write("-" * 30 + "\n")
            for param, value in report['optimized_config'].items():
                f.write(f"  {param}: {value}\n")
            f.write("\n")
            
            f.write("ğŸ“Š STATISTICHE PER FASE:\n")
            f.write("-" * 30 + "\n")
            for phase_name, stats in report['phase_statistics'].items():
                f.write(f"  {phase_name}:\n")
                f.write(f"    ğŸ§ª Test: {stats['successful_tests']}/{stats['tests_run']}\n")
                f.write(f"    ğŸ¯ Miglior accuracy: {stats['best_accuracy']:.6f}\n\n")
        
        print(f"ğŸ“„ Rapporto testuale: {report_file}")

def main():
    """Entry point del benchmark configurazione migliore."""
    parser = argparse.ArgumentParser(
        description='Benchmark Fine-Tuning Configurazione Migliore SNN-IDS',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Esempi di utilizzo:

  # Fine-tuning completo della configurazione migliore
  python3 benchmark-best-config.py

  # Con sample size personalizzato per risultati piÃ¹ stabili
  python3 benchmark-best-config.py --sample-size 30000

  # Solo test baseline per conferma
  python3 benchmark-best-config.py --baseline-only

  # Solo fine-tuning learning rate
  python3 benchmark-best-config.py --lr-only
        '''
    )
    
    parser.add_argument('--sample-size', type=int, 
                       help='Numero di campioni da utilizzare')
    parser.add_argument('--data-path', type=str, 
                       help='Path ai dati del dataset')
    parser.add_argument('--baseline-only', action='store_true',
                       help='Esegue solo il test baseline')
    parser.add_argument('--lr-only', action='store_true',
                       help='Esegue solo il fine-tuning del learning rate')
    
    args = parser.parse_args()
    
    try:
        benchmark = BestConfigBenchmark(
            sample_size=args.sample_size,
            data_path=args.data_path
        )
        
        if args.baseline_only:
            result = benchmark.test_baseline_config()
            if result.get('status') == 'success':
                print(f"âœ… Baseline confermato: {result['best_accuracy']:.6f}")
            return 0
        elif args.lr_only:
            results = benchmark.fine_tune_learning_rate()
            if results['best_result']:
                print(f"âœ… Miglior LR: {results['best_result']['best_accuracy']:.6f}")
            return 0
        else:
            # Fine-tuning completo
            results = benchmark.run_complete_fine_tuning()
            
            print(f"\nğŸ¯ Fine-tuning completato!")
            print(f"ğŸ“ˆ Miglioramento: +{results['improvement']['absolute']:.6f} ({results['improvement']['percent']:.2f}%)")
            print(f"ğŸ† Accuracy finale: {results['final_accuracy']:.6f}")
        
        return 0
        
    except Exception as e:
        print(f"\nâŒ Errore durante il fine-tuning: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())
