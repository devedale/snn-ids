{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Ambiente di Lavoro\n",
    "\n",
    "Prima di tutto, è **necessario** configurare l'ambiente di lavoro. La cella seguente clonerà il repository GitHub contenente tutti i moduli Python (`.py`) e si posizionerà nella directory corretta. \n",
    "\n",
    "**Esegui questa cella come primo passo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clona il repository e posizionati nella directory corretta\n",
    "# NOTA: Assicurati che l'URL del repository e il nome del branch siano corretti.\n",
    "!git clone https://github.com/devedale/snn-ids.git snn-ids\n",
    "%cd snn-ids\n",
    "# Assicurati di usare il nome del branch corretto.\n",
    "!git checkout feat/modular-ml-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manoscritto della Pipeline ML per Cybersecurity\n",
    "\n",
    "Questo notebook è una guida interattiva e un manoscritto completo per una pipeline di machine learning **avanzata** per l'analisi di traffico di rete. È stato progettato con tre principi chiave in mente:\n",
    "\n",
    "- **Modularità**: Ogni fase logica (preprocessing, training, predizione) è isolata nel proprio modulo Python (`.py`).\n",
    "- **Configurabilità**: Tutta la pipeline è controllata da un singolo file, `config.py`.\n",
    "- **Estensibilità**: La struttura è pensata per essere facilmente adattabile a nuovi dataset o a nuove architetture di modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architettura e Scelte Tecniche\n",
    "\n",
    "La pipeline è composta dai seguenti elementi:\n",
    "\n",
    "- `config.py`: Il cuore della configurazione. Qui si definiscono i percorsi, le colonne da usare e gli iperparametri per il training.\n",
    "- `preprocessing/process.py`: Contiene tutta la logica per caricare i dati e trasformarli in un formato numerico che il modello può comprendere.\n",
    "- `training/train.py`: Gestisce la creazione del modello, la grid search e il salvataggio del modello più performante.\n",
    "- `prediction/predict.py`: Carica un modello salvato e lo usa per fare predizioni su nuovi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Iniziale\n",
    "\n",
    "Installiamo le librerie necessarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa le dipendenze\n",
    "!pip install pandas scikit-learn tensorflow faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download ed Estrazione del Dataset\n",
    "\n",
    "La cella seguente si occupa di scaricare il dataset zippato da Kaggle e di estrarlo nella directory `data/`. Questo passaggio è fondamentale per avere i dati pronti per l'analisi.\n",
    "\n",
    "**Nota**: Questo comando utilizza `curl` e `unzip`, che sono standard in ambienti Linux come Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Downloads\n",
    "!curl -L -o ./Downloads/improved-cicids2017-and-csecicids2018.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/ernie55ernie/improved-cicids2017-and-csecicids2018\n",
    "\n",
    "!unzip -o ./Downloads/improved-cicids2017-and-csecicids2018.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica dell'Estrazione\n",
    "\n",
    "Eseguiamo un rapido controllo per assicurarci che i file siano stati estratti correttamente, visualizzando le prime righe di uno dei file CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 6 ./data/CSECICIDS2018_improved/Friday-02-03-2018.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adattamento della Configurazione\n",
    "\n",
    "Il file `config.py` è il cuore della nostra pipeline. Lo abbiamo già pre-configurato per funzionare con il nuovo dataset. Questa sezione spiega le modifiche chiave che sono state fatte. Puoi modificare `config.py` per sperimentare con diverse feature, iperparametri o strategie.\n",
    "\n",
    "In particolare, puoi cambiare il `model_type` in `\"gru\"` per utilizzare la nuova architettura di rete neurale che abbiamo aggiunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hub di Sperimentazione: Configura ed Esegui la Pipeline\n",
    "\n",
    "Questa sezione è il nuovo centro di controllo per tutti gli esperimenti. Invece di avere celle separate per lo smoke test e il training completo, ora abbiamo un approccio più flessibile e modulare. \n",
    "\n",
    "**Come funziona:**\n",
    "1.  **Definisci una Configurazione**: Nella cella seguente, puoi definire un dizionario Python chiamato `experiment_config`. Questo dizionario ti permette di sovrascrivere al volo qualsiasi parametro dai file di configurazione (`config.py`).\n",
    "2.  **Esegui la Pipeline**: La seconda cella leggerà la tua `experiment_config` e la userà per eseguire l'intera pipeline di preprocessing e training.\n",
    "\n",
    "Questo approccio ti permette di testare rapidamente diverse idee (nuovi modelli, strategie di validazione, iperparametri) semplicemente modificando il dizionario di configurazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CENTRO DI CONTROLLO ESPERIMENTI ---\n",
    "# Modifica questo dizionario per configurare la tua esecuzione.\n",
    "\n",
    "experiment_config = {\n",
    "    # Esempio 1: Smoke Test Rapido con GRU e Train/Test Split\n",
    "    \"PREPROCESSING_CONFIG\": {\n",
    "        \"sample_size\": 20000  # Usa solo le prime 20.000 righe\n",
    "    },\n",
    "    \"TRAINING_CONFIG\": {\n",
    "        \"model_type\": \"gru\", # Scegli tra 'lstm', 'gru', 'dense'\n",
    "        \"validation_strategy\": \"train_test_split\", # Scegli tra 'train_test_split', 'k_fold'\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": [5], # Poche epoche per un test veloce\n",
    "            \"batch_size\": [128],\n",
    "            \"learning_rate": [0.001],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"lstm_units\": [64],\n",
    "            \"gru_units\": [64]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Esempio 2: Training Completo con LSTM e K-Fold (commentato)\n",
    "    # Per usarlo, commenta l'Esempio 1 e de-commenta questo.\n",
    "    # \"PREPROCESSING_CONFIG\": {\n",
    "    #     \"sample_size\": None  # Usa l'intero dataset\n",
    "    # },\n",
    "    # \"TRAINING_CONFIG\": {\n",
    "    #     \"model_type\": \"lstm\",\n",
    "    #     \"validation_strategy\": \"k_fold\",\n",
    "    #     \"k_fold_splits\": 5,\n",
    "    #     \"hyperparameters\": {\n",
    "    #         \"epochs\": [20],\n",
    "    #         \"batch_size\": [64],\n",
    "    #         \"learning_rate": [0.001],\n",
    "    #         \"activation\": [\"relu\"],\n",
    "    #         \"lstm_units\": [64],\n",
    "    #         \"gru_units\": [64]\n",
    "    #     }\n",
    "    # }\n",
    "}\n",
    "\n",
    "print(\"Configurazione dell'esperimento caricata:\")\n",
    "import json\n",
    "print(json.dumps(experiment_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione della Pipeline\n",
    "\n",
    "Questa cella esegue la pipeline utilizzando la configurazione definita sopra. Non è necessario modificarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from preprocessing.process import preprocess_data\n",
    "from training.train import train_and_evaluate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Eseguiamo il preprocessing e il training con la configurazione specificata\n",
    "train_and_evaluate(config_override=experiment_config)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- ESECUZIONE PIPELINE COMPLETATA in {(end_time - start_time) / 60:.2f} minuti ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusioni e Prossimi Passi\n",
    "\n",
    "Questo notebook ha fornito una struttura completa e modulare per affrontare un problema di classificazione del traffico di rete utilizzando un dataset reale e complesso. Abbiamo visto come:\n",
    "\n",
    "1.  **Impostare l'ambiente** e scaricare un dataset di grandi dimensioni.\n",
    "2.  **Adattare la configurazione** della pipeline (`config.py`) per lavorare con le nuove feature.\n",
    "3.  **Eseguire uno \"Smoke Test\"** per verificare rapidamente la funzionalità end-to-end.\n",
    "4.  **Lanciare un training completo** per costruire un modello performante.\n",
    "\n",
    "### Prossimi Passi\n",
    "\n",
    "Da qui, ci sono molte direzioni interessanti da esplorare:\n",
    "\n",
    "-   **Sperimentazione con le Feature**: Prova a modificare la lista `feature_columns` in `config.py`. Aggiungere o rimuovere feature può avere un impatto significativo sulle performance.\n",
    "-   **Ottimizzazione degli Iperparametri**: Esegui una Grid Search più ampia modificando il dizionario `hyperparameters` in `config.py`. Prova diversi tassi di apprendimento, dimensioni dei batch o architetture di rete.\n",
    "-   **Analisi degli Errori**: Esamina i casi in cui il modello sbaglia le previsioni. Questo può rivelare pattern specifici che il modello fatica a riconoscere e suggerire nuove feature da creare.\n",
    "-   **Ricerca Accademica**: Come discusso, il prossimo passo potrebbe essere quello di cercare pubblicazioni che usano questo dataset e tentare di replicare le loro metodologie per confrontare i risultati.\n",
    "\n",
    "Buon lavoro!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
