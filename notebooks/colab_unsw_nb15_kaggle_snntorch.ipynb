{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clona la repo e installa requisiti\n",
        "!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\n",
        "%cd /content/snn-ids\n",
        "!pip -q install -r requirements.txt kaggle snntorch scikit-learn pyyaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN su UNSW-NB15 (Colab) — download via Kaggle e baseline snntorch\n",
        "\n",
        "Questo notebook scarica UNSW-NB15 da Kaggle (serve API `kaggle.json`), effettua preprocessing minimale e addestra una SNN semplice con snntorch.\n",
        "\n",
        "Istruzioni Kaggle:\n",
        "- Ottieni `kaggle.json` (Account → Create New API Token)\n",
        "- Caricalo in Colab e posizionalo in `/root/.kaggle/kaggle.json`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dipendenze e setup Kaggle\n",
        "!pip -q install kaggle snntorch pandas scikit-learn pyyaml\n",
        "import os, json\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# Se hai caricato kaggle.json nel tuo workspace Colab, copia il path qui sotto\n",
        "kaggle_src = '/content/kaggle.json'  # cambia se necessario\n",
        "if os.path.exists(kaggle_src):\n",
        "  !cp -f /content/kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json || true\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download UNSW-NB15 da Kaggle (sostituisci lo slug se differente)\n",
        "from pathlib import Path\n",
        "Path('/content/data').mkdir(exist_ok=True)\n",
        "!kaggle datasets download -d mOhammad/unsw-nb15 -p /content/data -q\n",
        "!unzip -q -o /content/data/*.zip -d /content/data\n",
        "!ls -lh /content/data | head -n 20\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Caricamento e preprocessing minimo (adatta ai file estratti)\n",
        "import pandas as pd, numpy as np\n",
        "from glob import glob\n",
        "files = sorted(glob('/content/data/**/*.csv', recursive=True)) or sorted(glob('/content/data/*.csv'))\n",
        "print(len(files), 'csv trovati')\n",
        "\n",
        "# Esempio: concatena tutti i CSV disponibili (alcuni release hanno train/test separati)\n",
        "dfs = []\n",
        "for f in files[:5]:  # limita per demo se necessario\n",
        "  try:\n",
        "    df = pd.read_csv(f)\n",
        "    dfs.append(df)\n",
        "  except Exception as e:\n",
        "    print('skip', f, e)\n",
        "\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "print(df.shape)\n",
        "\n",
        "# Colonna target (adatta se differente nella tua versione)\n",
        "label_col = 'label' if 'label' in df.columns else ('Label' if 'Label' in df.columns else None)\n",
        "assert label_col is not None, 'Impossibile trovare colonna etichetta (label/Label)'\n",
        "\n",
        "# Binarizza: normal/benign=0, attack/altri=1\n",
        "labels = df[label_col].astype(str).str.lower()\n",
        "y = (~labels.str.contains('normal') & ~labels.str.contains('benign')).astype(int).values\n",
        "\n",
        "# Selezione feature numeriche + one-hot su poche categoriali\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_candidates = [c for c in df.columns if df[c].dtype == object and c != label_col]\n",
        "cat_cols = cat_candidates[:3]  # poche per demo\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_num = scaler.fit_transform(df[num_cols].fillna(0).values.astype(float)) if num_cols else np.zeros((len(df),0))\n",
        "X_cat = pd.get_dummies(df[cat_cols].fillna('NA').astype(str)).values if cat_cols else np.zeros((len(df),0))\n",
        "X = np.hstack([X_num, X_cat]).astype(np.float32)\n",
        "\n",
        "# Train/test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "input_size, output_size = X_train.shape[1], 2\n",
        "X_train.shape, X_test.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SNN snntorch (rate coding) — identico al notebook NSL-KDD\n",
        "import torch, torch.nn as nn, snntorch as snn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "timesteps = 10\n",
        "\n",
        "def rate_encode(X, timesteps=10):\n",
        "  Xc = torch.tensor(X, device=device)\n",
        "  return Xc.unsqueeze(1).repeat(1, timesteps, 1)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(rate_encode(X_train,timesteps), torch.tensor(y_train,device=device)), batch_size=256, shuffle=True)\n",
        "test_loader  = DataLoader(TensorDataset(rate_encode(X_test, timesteps), torch.tensor(y_test, device=device)), batch_size=512, shuffle=False)\n",
        "\n",
        "class SNNNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden=256, output_size=2):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden)\n",
        "    self.lif1 = snn.Leaky(beta=0.9)\n",
        "    self.fc2 = nn.Linear(hidden, output_size)\n",
        "    self.lif2 = snn.Leaky(beta=0.9)\n",
        "  def forward(self, x):\n",
        "    mem1 = self.lif1.init_leaky(); mem2 = self.lif2.init_leaky()\n",
        "    spk_sum = 0\n",
        "    for t in range(x.size(1)):\n",
        "      cur = self.fc1(x[:, t, :])\n",
        "      spk1, mem1 = self.lif1(cur, mem1)\n",
        "      cur2 = self.fc2(spk1)\n",
        "      spk2, mem2 = self.lif2(cur2, mem2)\n",
        "      spk_sum = spk_sum + spk2\n",
        "    return spk_sum\n",
        "\n",
        "model = SNNNet(input_size, 256, output_size).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training + valutazione + salvataggio\n",
        "import numpy as np\n",
        "\n",
        "def accuracy(logits, y):\n",
        "  preds = torch.argmax(logits, dim=1)\n",
        "  return (preds == y).float().mean().item()\n",
        "\n",
        "for epoch in range(5):\n",
        "  model.train(); losses=[]; accs=[]\n",
        "  for Xb, yb in train_loader:\n",
        "    opt.zero_grad(); logits = model(Xb)\n",
        "    loss = criterion(logits, yb)\n",
        "    loss.backward(); opt.step()\n",
        "    losses.append(loss.item()); accs.append(accuracy(logits, yb))\n",
        "  print(f\"Epoch {epoch+1}: loss={np.mean(losses):.4f}, acc={np.mean(accs):.3f}\")\n",
        "\n",
        "model.eval(); accs=[]\n",
        "with torch.no_grad():\n",
        "  for Xb, yb in test_loader:\n",
        "    logits = model(Xb)\n",
        "    accs.append(accuracy(logits, yb))\n",
        "print(f\"Test Accuracy: {np.mean(accs):.3f}\")\n",
        "\n",
        "from pathlib import Path\n",
        "Path('/content/out').mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), \"/content/out/snn_unsw_nb15.pt\")\n",
        "!ls -lh /content/out\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
