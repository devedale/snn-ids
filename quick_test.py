#!/usr/bin/env python3
"""
Test Rapido SNN Security Analytics
Esecuzione completa in locale con dati ridotti per valutazione veloce
"""

import logging
import time
import json
from pathlib import Path

# Setup logging minimale
logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')

def quick_test():
    """Test completo rapido del sistema"""
    print("ğŸš€ SNN Security Analytics - Test Rapido")
    print("="*50)
    
    start_time = time.time()
    
    try:
        # Import moduli
        print("ğŸ“¦ Caricamento moduli...")
        from pipeline_orchestrator import PipelineOrchestrator, PipelineConfig
        
        # Configurazione rapida
        config = PipelineConfig(
            training_config_file="training_config.example.yaml",  # YAML di esempio
            training_framework="mock",  # Fallback se YAML assente
            training_epochs=5,          # Override minimo per velocitÃ 
            enable_reconstruction=True,
            anomaly_threshold=0.7,      # Soglia permissiva per vedere anomalie
            secure_export=False         # Salta export sicuro per velocitÃ 
        )
        
        orchestrator = PipelineOrchestrator(config)
        
        # File di test esistenti
        test_files = [
            "input/FGT80FTK22013405.root.tlog.txt",
            "input/FGT80FTK22013405.root.elog.txt"
        ]
        
        # Verifica file esistenti
        existing_files = [f for f in test_files if Path(f).exists()]
        if not existing_files:
            print("âŒ Nessun file di test trovato in input/")
            return False
        
        print(f"ğŸ“„ Usando {len(existing_files)} file di test")
        
        # 1. PREPROCESSING RAPIDO
        print("\nğŸ”„ Fase 1: Preprocessing...")
        prep_start = time.time()
        
        try:
            prep_results = orchestrator.run_preprocessing_only(existing_files)
            prep_time = time.time() - prep_start
            
            print(f"âœ… Preprocessing completato in {prep_time:.1f}s")
            print(f"   ğŸ“Š {prep_results['total_logs']} log processati")
            print(f"   ğŸ§® {prep_results['snn_samples']} campioni SNN")
            print(f"   ğŸ“ˆ {prep_results['snn_features']} features")
            
        except Exception as e:
            print(f"âŒ Errore preprocessing: {e}")
            return False
        
        # 2. TRAINING RAPIDO
        print("\nğŸ§  Fase 2: Training SNN...")
        train_start = time.time()
        
        try:
            snn_dataset = prep_results['output_files']['snn_dataset']
            train_results = orchestrator.run_training_only(snn_dataset)
            train_time = time.time() - train_start
            
            print(f"âœ… Training completato in {train_time:.1f}s")
            training_history = train_results['training_results']['training_history']
            print(f"   ğŸ¯ Framework: {training_history['framework']}")
            print(f"   ğŸ“‰ Final loss: {training_history['final_loss']:.4f}")
            
        except Exception as e:
            print(f"âŒ Errore training: {e}")
            return False
        
        # 3. ANALYSIS RAPIDO
        print("\nğŸ“Š Fase 3: Analisi risultati...")
        analysis_start = time.time()
        
        try:
            # Trova file mapping automaticamente
            mapping_file = None
            norm_file = None
            
            output_dir = Path("output")
            for f in output_dir.glob("*mapping*.json"):
                mapping_file = str(f)
                break
            for f in output_dir.glob("*normalization*.json"):
                norm_file = str(f)
                break
            
            # Usa il dataset SNN reale invece di dummy per l'analisi
            snn_dataset = prep_results['output_files']['snn_dataset']
            
            analysis_results = orchestrator.run_analysis_only(
                train_results['export_path'],
                "dummy_predictions.json",  # Simula predizioni (non serve file esistente)
                mapping_file,
                norm_file
            )
            analysis_time = time.time() - analysis_start
            
            print(f"âœ… Analisi completata in {analysis_time:.1f}s")
            
            # Risultati analisi
            anomaly_summary = analysis_results['anomaly_summary']
            print(f"   ğŸš¨ {anomaly_summary['total_anomalies']} anomalie rilevate")
            print(f"   ğŸ“ˆ Tasso anomalie: {anomaly_summary['anomaly_rate']:.1%}")
            print(f"   ğŸ¯ Confidence media: {anomaly_summary.get('avg_anomaly_confidence', 0):.3f}")
            
            # Log ricostruiti
            reconstructed = analysis_results['reconstructed_logs']
            if reconstructed:
                print(f"   ğŸ”„ {len(reconstructed)} log ricostruiti")
            
        except Exception as e:
            print(f"âŒ Errore analisi: {e}")
            return False
        
        # 4. RISULTATI FINALI
        total_time = time.time() - start_time
        
        print("\n" + "="*50)
        print("ğŸ‰ TEST COMPLETATO CON SUCCESSO!")
        print("="*50)
        print(f"â±ï¸  Tempo totale: {total_time:.1f} secondi")
        print(f"ğŸ“Š Log processati: {prep_results['total_logs']}")
        print(f"ğŸ§® Campioni SNN: {prep_results['snn_samples']}")
        print(f"ğŸš¨ Anomalie: {anomaly_summary['total_anomalies']}")
        
        # Mostra file generati
        print(f"\nğŸ“‚ File generati in output/:")
        output_files = list(Path("output").glob("*"))
        for f in sorted(output_files)[-5:]:  # Ultimi 5 file
            size = f.stat().st_size / 1024  # KB
            print(f"   ğŸ“„ {f.name} ({size:.1f} KB)")
        
        # Suggerimenti
        print("\nğŸ’¡ Cosa puoi fare ora:")
        print("   ğŸ“‹ Visualizza report: cat output/analysis_report.json")
        print("   ğŸ“Š Visualizza dataset: head output/*snn_dataset.csv")
        print("   ğŸ” Visualizza anomalie: head output/analysis_report.json")
        
        return True
        
    except ImportError as e:
        print(f"âŒ Errore import: {e}")
        print("ğŸ’¡ Prova: pip install -r requirements.txt")
        return False
    except Exception as e:
        print(f"âŒ Errore generale: {e}")
        import traceback
        traceback.print_exc()
        return False

def show_sample_results():
    """Mostra campioni dei risultati generati"""
    print("\nğŸ” CAMPIONI RISULTATI:")
    print("="*30)
    
    # Mostra sample dataset SNN
    try:
        import pandas as pd
        
        snn_files = list(Path("output").glob("*snn_dataset.csv"))
        if snn_files:
            df = pd.read_csv(snn_files[0])
            print(f"ğŸ“Š Dataset SNN ({len(df)} righe, {len(df.columns)} colonne):")
            print(df.head(3).to_string(index=False))
        
        # Mostra sample report
        report_files = list(Path("output").glob("*analysis_report.json"))
        if report_files:
            with open(report_files[0]) as f:
                report = json.load(f)
            
            print(f"\nğŸ“‹ Report Analisi:")
            summary = report.get('analysis_summary', {})
            print(f"   â€¢ Campioni totali: {summary.get('total_samples', 0)}")
            print(f"   â€¢ Anomalie rilevate: {summary.get('anomalies_detected', 0)}")
            print(f"   â€¢ Tasso anomalie: {summary.get('anomaly_rate', 0):.1%}")
            
    except Exception as e:
        print(f"Errore visualizzazione: {e}")

if __name__ == "__main__":
    print("ğŸ§ª Avvio test rapido del sistema SNN...")
    
    success = quick_test()
    
    if success:
        show_sample_results()
        print("\nğŸ¯ Sistema funzionante e pronto per uso avanzato!")
    else:
        print("\nâŒ Test fallito - controlla la configurazione")
    
    print(f"\nğŸ“š Per informazioni dettagliate: cat USAGE_GUIDE.md")
