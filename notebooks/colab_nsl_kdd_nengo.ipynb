{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN (Nengo/NengoDL) su NSL-KDD — Colab\n",
        "\n",
        "Notebook end-to-end: clona la repo, installa requisiti, scarica NSL-KDD, preprocessing, rete Nengo con neuroni LIF, training con NengoDL, valutazione e salvataggio parametri.\n",
        "\n",
        "- Repo: https://github.com/devedale/snn-ids\n",
        "- Framework: Nengo + NengoDL (TensorFlow backend)\n",
        "- Codifica: rate coding (ripetizione su T step)\n",
        "- Task: binaria (anomalo vs normale)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clona la repo e installa requisiti\n",
        "!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\n",
        "%cd /content/snn-ids\n",
        "!pip -q install -r requirements.txt nengo nengo-dl scikit-learn pyyaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installazione dipendenze Nengo/NengoDL\n",
        "!pip -q install nengo nengo-dl pandas scikit-learn pyyaml\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NSL-KDD (train/test) e conversione CSV→dataset SNN via repo\n",
        "base = Path('/content/data'); base.mkdir(parents=True, exist_ok=True)\n",
        "urls = {\n",
        "    'KDDTrain+': 'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt',\n",
        "    'KDDTest+':  'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt',\n",
        "}\n",
        "for name, url in urls.items():\n",
        "    !wget -q -O /content/data/{name}.txt \"{url}\"\n",
        "!python -m tools.csv_to_snn_dataset --input /content/data/KDDTrain+.txt --output /content/data/nslkdd_snn_train.csv --label-col label\n",
        "!python -m tools.csv_to_snn_dataset --input /content/data/KDDTest+.txt --output /content/data/nslkdd_snn_test.csv --label-col label\n",
        "!ls -lh /content/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carica dataset SNN già convertiti (train/test) e label reali dai raw\n",
        "train_df = pd.read_csv('/content/data/nslkdd_snn_train.csv')\n",
        "test_df  = pd.read_csv('/content/data/nslkdd_snn_test.csv')\n",
        "\n",
        "# Label da file raw\n",
        "cols = [\n",
        "  \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\n",
        "  \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "  \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\n",
        "  \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "  \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
        "  \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
        "  \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"\n",
        "]\n",
        "raw_train = pd.read_csv('/content/data/KDDTrain+.txt', names=cols)\n",
        "raw_test  = pd.read_csv('/content/data/KDDTest+.txt',  names=cols)\n",
        "y_train = (raw_train['label'].astype(str).str.lower() != 'normal').astype(int).values\n",
        "y_test  = (raw_test['label'].astype(str).str.lower()  != 'normal').astype(int).values\n",
        "\n",
        "# Estrai features\n",
        "feat_cols = [c for c in train_df.columns if c != 'timestamp']\n",
        "X_train = train_df[feat_cols].values.astype(np.float32)\n",
        "X_test  = test_df[feat_cols].values.astype(np.float32)\n",
        "input_size, output_size = X_train.shape[1], 2\n",
        "X_train.shape, X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Costruzione rete Nengo e training con NengoDL\n",
        "import nengo, nengo_dl, tensorflow as tf\n",
        "\n",
        "timesteps = 10\n",
        "with nengo.Network() as net:\n",
        "  inp = nengo.Node(np.zeros(input_size))\n",
        "  ens1 = nengo.Ensemble(n_neurons=256, dimensions=input_size, neuron_type=nengo.LIF())\n",
        "  nengo.Connection(inp, ens1)\n",
        "  ens2 = nengo.Ensemble(n_neurons=output_size, dimensions=output_size, neuron_type=nengo.LIF())\n",
        "  nengo.Connection(ens1, ens2)\n",
        "  p_out = nengo.Probe(ens2, synapse=0.1)\n",
        "\n",
        "# rate coding per NengoDL: ripeti features su T\n",
        "X_train_enc = np.repeat(X_train[:, None, :], timesteps, axis=1)\n",
        "X_test_enc  = np.repeat(X_test[:,  None, :], timesteps, axis=1)\n",
        "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=output_size)\n",
        "y_test_oh  = tf.keras.utils.to_categorical(y_test,  num_classes=output_size)\n",
        "y_train_enc = np.repeat(y_train_oh[:, None, :], timesteps, axis=1)\n",
        "y_test_enc  = np.repeat(y_test_oh[:,  None, :], timesteps, axis=1)\n",
        "\n",
        "minibatch = 128\n",
        "with nengo_dl.Simulator(net, minibatch_size=minibatch) as sim:\n",
        "  # input mapping: primo input del modello → X\n",
        "  inputs = {inp: X_train_enc}\n",
        "  targets = {p_out: y_train_enc}\n",
        "  sim.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.MSE)\n",
        "  sim.fit(inputs, targets, epochs=5, validation_split=0.2, verbose=2)\n",
        "  # valutazione\n",
        "  test_dict = {inp: X_test_enc}\n",
        "  sim.run_steps(X_test_enc.shape[1], data=test_dict)\n",
        "  out = sim.data[p_out]\n",
        "\n",
        "pred = out.mean(axis=1)\n",
        "acc = (pred.argmax(axis=1) == y_test).mean()\n",
        "print('Test accuracy:', acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvataggio artefatti (parametri rete)\n",
        "import pickle, os\n",
        "os.makedirs('/content/out', exist_ok=True)\n",
        "with open('/content/out/nengo_nslkdd.pkl', 'wb') as f:\n",
        "  pickle.dump(net, f)\n",
        "print('Salvato /content/out/nengo_nslkdd.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
