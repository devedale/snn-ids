{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ BENCHMARK COLAB - Iperparametri Significativi\n",
        "# Configurazione ottimizzata per Google Colab con ~50 unit√† di calcolo\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurazione benchmark per Colab\n",
        "COLAB_BENCHMARK_CONFIG = {\n",
        "    # Dataset ridotto per velocit√†\n",
        "    \"sample_size\": 8000,  # Bilanciato tra velocit√† e significativit√†\n",
        "    \n",
        "    # Finestre temporali significative per cybersecurity\n",
        "    \"time_windows\": [\n",
        "        {\"window_size\": 30, \"step\": 15, \"name\": \"30s_15s\"},    # Attacchi rapidi\n",
        "        {\"window_size\": 60, \"step\": 30, \"name\": \"1m_30s\"},     # Sessioni brevi\n",
        "        {\"window_size\": 300, \"step\": 150, \"name\": \"5m_2.5m\"},  # Sessioni lunghe\n",
        "    ],\n",
        "    \n",
        "    # Architetture da testare\n",
        "    \"models\": [\n",
        "        {\n",
        "            \"type\": \"dense\", \n",
        "            \"name\": \"Dense_Baseline\",\n",
        "            \"params\": {\"epochs\": 3, \"batch_size\": 64}\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"gru\", \n",
        "            \"name\": \"GRU_Fast\",\n",
        "            \"params\": {\"epochs\": 3, \"batch_size\": 32, \"gru_units\": 64}\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"lstm\", \n",
        "            \"name\": \"LSTM_Optimized\",\n",
        "            \"params\": {\"epochs\": 3, \"batch_size\": 32, \"lstm_units\": 64}\n",
        "        }\n",
        "    ],\n",
        "    \n",
        "    # Learning rates significativi\n",
        "    \"learning_rates\": [0.001, 0.0005],  # Solo i pi√π promettenti\n",
        "    \n",
        "    # Aggregazioni statistiche\n",
        "    \"aggregation_stats\": [\n",
        "        [\"sum\", \"mean\", \"max\"],           # Base veloce\n",
        "        [\"sum\", \"mean\", \"std\", \"max\"],   # Completa\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"üéØ BENCHMARK COLAB CONFIGURATO\")\n",
        "print(f\"üìä Sample size: {COLAB_BENCHMARK_CONFIG['sample_size']:,}\")\n",
        "print(f\"‚è±Ô∏è Finestre temporali: {len(COLAB_BENCHMARK_CONFIG['time_windows'])}\")\n",
        "print(f\"ü§ñ Modelli: {len(COLAB_BENCHMARK_CONFIG['models'])}\")\n",
        "print(f\"üìà Learning rates: {COLAB_BENCHMARK_CONFIG['learning_rates']}\")\n",
        "print(f\"üìä Configurazioni aggregazione: {len(COLAB_BENCHMARK_CONFIG['aggregation_stats'])}\")\n",
        "\n",
        "# Stima tempo totale\n",
        "total_configs = (len(COLAB_BENCHMARK_CONFIG['time_windows']) * \n",
        "                len(COLAB_BENCHMARK_CONFIG['models']) * \n",
        "                len(COLAB_BENCHMARK_CONFIG['learning_rates']) * \n",
        "                len(COLAB_BENCHMARK_CONFIG['aggregation_stats']))\n",
        "\n",
        "print(f\"üî¢ Configurazioni totali: {total_configs}\")\n",
        "print(f\"‚è∞ Tempo stimato: ~{total_configs * 2:.0f}-{total_configs * 4:.0f} minuti\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Funzione di Benchmark Automatizzato\n",
        "\n",
        "def run_colab_benchmark():\n",
        "    \"\"\"\n",
        "    Esegue benchmark completo con configurazioni ottimizzate per Colab.\n",
        "    Testa combinazioni significative di iperparametri.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Import necessari\n",
        "    from preprocessing.process import preprocess_pipeline\n",
        "    from training.train import train_model\n",
        "    from evaluation.evaluate import evaluate_model_comprehensive\n",
        "    from config import PREPROCESSING_CONFIG, TRAINING_CONFIG\n",
        "    \n",
        "    # Risultati\n",
        "    all_results = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(\"üöÄ AVVIO BENCHMARK COLAB\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    config_num = 0\n",
        "    total_configs = (len(COLAB_BENCHMARK_CONFIG['time_windows']) * \n",
        "                    len(COLAB_BENCHMARK_CONFIG['models']) * \n",
        "                    len(COLAB_BENCHMARK_CONFIG['learning_rates']) * \n",
        "                    len(COLAB_BENCHMARK_CONFIG['aggregation_stats']))\n",
        "    \n",
        "    # Loop attraverso tutte le configurazioni\n",
        "    for window_config in COLAB_BENCHMARK_CONFIG['time_windows']:\n",
        "        for model_config in COLAB_BENCHMARK_CONFIG['models']:\n",
        "            for lr in COLAB_BENCHMARK_CONFIG['learning_rates']:\n",
        "                for agg_stats in COLAB_BENCHMARK_CONFIG['aggregation_stats']:\n",
        "                    \n",
        "                    config_num += 1\n",
        "                    config_start = time.time()\n",
        "                    \n",
        "                    print(f\"\\nüß™ CONFIGURAZIONE {config_num}/{total_configs}\")\n",
        "                    print(f\"‚è±Ô∏è Finestra: {window_config['name']}\")\n",
        "                    print(f\"ü§ñ Modello: {model_config['name']}\")\n",
        "                    print(f\"üìà Learning Rate: {lr}\")\n",
        "                    print(f\"üìä Aggregazioni: {agg_stats}\")\n",
        "                    print(\"-\" * 40)\n",
        "                    \n",
        "                    try:\n",
        "                        # 1. Configura preprocessing\n",
        "                        PREPROCESSING_CONFIG.update({\n",
        "                            'sample_size': COLAB_BENCHMARK_CONFIG['sample_size'],\n",
        "                            'window_size': window_config['window_size'],\n",
        "                            'step': window_config['step'],\n",
        "                            'aggregation_stats': agg_stats\n",
        "                        })\n",
        "                        \n",
        "                        # 2. Configura training\n",
        "                        TRAINING_CONFIG.update({\n",
        "                            'model_type': model_config['type'],\n",
        "                            'hyperparameters': {\n",
        "                                **model_config['params'],\n",
        "                                'learning_rate': [lr]\n",
        "                            }\n",
        "                        })\n",
        "                        \n",
        "                        # 3. Preprocessing\n",
        "                        print(\"üîÑ Preprocessing...\")\n",
        "                        X, y, label_encoder = preprocess_pipeline()\n",
        "                        \n",
        "                        if X.shape[0] == 0:\n",
        "                            print(\"‚ùå Nessun dato dopo preprocessing\")\n",
        "                            continue\n",
        "                        \n",
        "                        # 4. Training\n",
        "                        print(\"üèãÔ∏è Training...\")\n",
        "                        model, training_log, model_path = train_model(\n",
        "                            X, y, \n",
        "                            model_type=model_config['type']\n",
        "                        )\n",
        "                        \n",
        "                        # 5. Valutazione rapida\n",
        "                        print(\"üìä Valutazione...\")\n",
        "                        from sklearn.model_selection import train_test_split\n",
        "                        X_train, X_test, y_train, y_test = train_test_split(\n",
        "                            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "                        )\n",
        "                        \n",
        "                        eval_results = evaluate_model_comprehensive(\n",
        "                            model, X_test, y_test, \n",
        "                            class_names=label_encoder.classes_.tolist(),\n",
        "                            output_dir=f'colab_eval/config_{config_num}'\n",
        "                        )\\n                        \n",
        "                        # 6. Salva risultati\n",
        "                        config_time = time.time() - config_start\n",
        "                        result = {\n",
        "                            'config_id': config_num,\n",
        "                            'window_config': window_config,\n",
        "                            'model_config': model_config,\n",
        "                            'learning_rate': lr,\n",
        "                            'aggregation_stats': agg_stats,\n",
        "                            'data_shape': {'X': X.shape, 'y': y.shape},\n",
        "                            'accuracy': eval_results['basic_metrics']['accuracy'],\n",
        "                            'f1_macro': eval_results['basic_metrics']['f1_macro'],\n",
        "                            'training_time': training_log.get('training_time', 0),\n",
        "                            'config_time': config_time,\n",
        "                            'model_path': model_path\n",
        "                        }\n",
        "                        \n",
        "                        all_results.append(result)\n",
        "                        \n",
        "                        print(f\"‚úÖ Completato in {config_time:.1f}s\")\n",
        "                        print(f\"üìä Accuracy: {result['accuracy']:.3f}\")\n",
        "                        print(f\"üìä F1-Macro: {result['f1_macro']:.3f}\")\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå Errore configurazione {config_num}: {e}\")\n",
        "                        continue\n",
        "    \n",
        "    # Risultati finali\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nüéâ BENCHMARK COMPLETATO!\")\n",
        "    print(f\"‚è∞ Tempo totale: {total_time/60:.1f} minuti\")\n",
        "    print(f\"‚úÖ Configurazioni completate: {len(all_results)}/{total_configs}\")\n",
        "    \n",
        "    # Salva risultati\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_file = f\"colab_benchmark_results_{timestamp}.json\"\n",
        "    \n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'timestamp': timestamp,\n",
        "            'total_time': total_time,\n",
        "            'total_configs': total_configs,\n",
        "            'completed_configs': len(all_results),\n",
        "            'results': all_results\n",
        "        }, f, indent=2, default=str)\n",
        "    \n",
        "    print(f\"üíæ Risultati salvati: {results_file}\")\n",
        "    \n",
        "    return all_results, results_file\n",
        "\n",
        "# Preparazione per esecuzione\n",
        "print(\"üéØ Funzione benchmark pronta!\")\n",
        "print(\"üí° Per eseguire: all_results, results_file = run_colab_benchmark()\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Analisi e Visualizzazione Risultati\n",
        "\n",
        "def analyze_benchmark_results(results_file):\n",
        "    \"\"\"\n",
        "    Analizza i risultati del benchmark e crea visualizzazioni.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    \n",
        "    # Carica risultati\n",
        "    with open(results_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    results = data['results']\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    print(f\"üìä ANALISI RISULTATI BENCHMARK\")\n",
        "    print(f\"‚è∞ Tempo totale: {data['total_time']/60:.1f} minuti\")\n",
        "    print(f\"‚úÖ Configurazioni completate: {data['completed_configs']}/{data['total_configs']}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Migliori configurazioni\n",
        "    print(\"\\nüèÜ TOP 5 CONFIGURAZIONI (Accuracy)\")\n",
        "    top_configs = df.nlargest(5, 'accuracy')[['config_id', 'accuracy', 'f1_macro', 'window_config', 'model_config']]\n",
        "    for _, row in top_configs.iterrows():\n",
        "        print(f\"  Config {row['config_id']}: {row['accuracy']:.3f} acc, {row['f1_macro']:.3f} f1\")\n",
        "        print(f\"    ü™ü {row['window_config']['name']} | ü§ñ {row['model_config']['name']}\")\n",
        "    \n",
        "    # 2. Analisi per finestra temporale\n",
        "    print(\"\\n‚è±Ô∏è PERFORMANCE PER FINESTRA TEMPORALE\")\n",
        "    window_stats = df.groupby(df['window_config'].apply(lambda x: x['name'])).agg({\n",
        "        'accuracy': ['mean', 'std', 'max'],\n",
        "        'f1_macro': ['mean', 'std', 'max'],\n",
        "        'config_time': 'mean'\n",
        "    }).round(3)\n",
        "    print(window_stats)\n",
        "    \n",
        "    # 3. Analisi per modello\n",
        "    print(\"\\nü§ñ PERFORMANCE PER MODELLO\")\n",
        "    model_stats = df.groupby(df['model_config'].apply(lambda x: x['name'])).agg({\n",
        "        'accuracy': ['mean', 'std', 'max'],\n",
        "        'f1_macro': ['mean', 'std', 'max'],\n",
        "        'config_time': 'mean'\n",
        "    }).round(3)\n",
        "    print(model_stats)\n",
        "    \n",
        "    # 4. Visualizzazioni\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Accuracy per finestra\n",
        "    window_names = [x['name'] for x in df['window_config']]\n",
        "    df['window_name'] = window_names\n",
        "    sns.boxplot(data=df, x='window_name', y='accuracy', ax=axes[0,0])\n",
        "    axes[0,0].set_title('Accuracy per Finestra Temporale')\n",
        "    axes[0,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Accuracy per modello\n",
        "    model_names = [x['name'] for x in df['model_config']]\n",
        "    df['model_name'] = model_names\n",
        "    sns.boxplot(data=df, x='model_name', y='accuracy', ax=axes[0,1])\n",
        "    axes[0,1].set_title('Accuracy per Modello')\n",
        "    axes[0,1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Tempo vs Accuracy\n",
        "    axes[1,0].scatter(df['config_time'], df['accuracy'], alpha=0.7)\n",
        "    axes[1,0].set_xlabel('Tempo Configurazione (s)')\n",
        "    axes[1,0].set_ylabel('Accuracy')\n",
        "    axes[1,0].set_title('Tempo vs Accuracy')\n",
        "    \n",
        "    # Learning Rate vs Accuracy\n",
        "    sns.boxplot(data=df, x='learning_rate', y='accuracy', ax=axes[1,1])\n",
        "    axes[1,1].set_title('Learning Rate vs Accuracy')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'benchmark_analysis_{data[\"timestamp\"]}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # 5. Raccomandazioni\n",
        "    print(\"\\nüí° RACCOMANDAZIONI\")\n",
        "    best_config = df.loc[df['accuracy'].idxmax()]\n",
        "    print(f\"ü•á Migliore configurazione: Config {best_config['config_id']}\")\n",
        "    print(f\"   üìä Accuracy: {best_config['accuracy']:.3f}\")\n",
        "    print(f\"   ü™ü Finestra: {best_config['window_config']['name']}\")\n",
        "    print(f\"   ü§ñ Modello: {best_config['model_config']['name']}\")\n",
        "    print(f\"   üìà Learning Rate: {best_config['learning_rate']}\")\n",
        "    \n",
        "    # Efficienza (accuracy/tempo)\n",
        "    df['efficiency'] = df['accuracy'] / df['config_time']\n",
        "    most_efficient = df.loc[df['efficiency'].idxmax()]\n",
        "    print(f\"\\n‚ö° Configurazione pi√π efficiente: Config {most_efficient['config_id']}\")\n",
        "    print(f\"   üìä Accuracy: {most_efficient['accuracy']:.3f}\")\n",
        "    print(f\"   ‚è±Ô∏è Tempo: {most_efficient['config_time']:.1f}s\")\n",
        "    print(f\"   üéØ Efficienza: {most_efficient['efficiency']:.4f} acc/s\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"üìä Funzione di analisi pronta!\")\n",
        "print(\"üí° Dopo il benchmark: df = analyze_benchmark_results(results_file)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ ESECUZIONE BENCHMARK COLAB\n",
        "# Esegui questa cella per avviare il benchmark completo\n",
        "\n",
        "print(\"üéØ AVVIO BENCHMARK OTTIMIZZATO PER COLAB\")\n",
        "print(\"‚ö° Configurazione: 36 test totali (~60-120 minuti)\")\n",
        "print(\"üî• GPU consigliata per velocit√† ottimale\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Esegui il benchmark\n",
        "all_results, results_file = run_colab_benchmark()\n",
        "\n",
        "print(f\"\\nüéâ BENCHMARK COMPLETATO!\")\n",
        "print(f\"üìÅ File risultati: {results_file}\")\n",
        "print(f\"üìä Configurazioni testate: {len(all_results)}\")\n",
        "\n",
        "# Analisi automatica\n",
        "print(\"\\nüîç AVVIO ANALISI RISULTATI...\")\n",
        "df_results = analyze_benchmark_results(results_file)\n",
        "\n",
        "print(f\"\\n‚úÖ TUTTO COMPLETATO!\")\n",
        "print(f\"üíæ Risultati salvati in: {results_file}\")\n",
        "print(f\"üìà Grafici salvati come: benchmark_analysis_*.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Test Rapido di una Singola Configurazione\n",
        "# Usa questa cella per testare rapidamente una configurazione specifica\n",
        "\n",
        "def quick_test_config(window_size=60, step=30, model_type='gru', learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Test rapido di una configurazione specifica.\n",
        "    \"\"\"\n",
        "    from preprocessing.process import preprocess_pipeline\n",
        "    from training.train import train_model\n",
        "    from evaluation.evaluate import evaluate_model_comprehensive\n",
        "    from config import PREPROCESSING_CONFIG, TRAINING_CONFIG\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    \n",
        "    print(f\"üß™ TEST RAPIDO\")\n",
        "    print(f\"‚è±Ô∏è Finestra: {window_size}s (step: {step}s)\")\n",
        "    print(f\"ü§ñ Modello: {model_type}\")\n",
        "    print(f\"üìà Learning Rate: {learning_rate}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        # Configura\n",
        "        PREPROCESSING_CONFIG.update({\n",
        "            'sample_size': 3000,  # Ridotto per velocit√†\n",
        "            'window_size': window_size,\n",
        "            'step': step,\n",
        "            'aggregation_stats': [\"sum\", \"mean\", \"max\"]\n",
        "        })\n",
        "        \n",
        "        TRAINING_CONFIG.update({\n",
        "            'model_type': model_type,\n",
        "            'hyperparameters': {\n",
        "                'epochs': [2],  # Ridotto per velocit√†\n",
        "                'batch_size': [32],\n",
        "                'learning_rate': [learning_rate]\n",
        "            }\n",
        "        })\n",
        "        \n",
        "        # Pipeline\n",
        "        print(\"üîÑ Preprocessing...\")\n",
        "        X, y, label_encoder = preprocess_pipeline()\n",
        "        \n",
        "        print(\"üèãÔ∏è Training...\")\n",
        "        model, training_log, model_path = train_model(X, y, model_type=model_type)\n",
        "        \n",
        "        print(\"üìä Valutazione...\")\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        \n",
        "        eval_results = evaluate_model_comprehensive(\n",
        "            model, X_test, y_test, \n",
        "            class_names=label_encoder.classes_.tolist(),\n",
        "            output_dir='quick_test_eval'\n",
        "        )\n",
        "        \n",
        "        total_time = time.time() - start_time\n",
        "        \n",
        "        print(f\"\\n‚úÖ TEST COMPLETATO in {total_time:.1f}s\")\n",
        "        print(f\"üìä Accuracy: {eval_results['basic_metrics']['accuracy']:.3f}\")\n",
        "        print(f\"üìä F1-Macro: {eval_results['basic_metrics']['f1_macro']:.3f}\")\n",
        "        print(f\"üìä Shape dati: X={X.shape}, y={y.shape}\")\n",
        "        \n",
        "        return eval_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"üß™ Funzione test rapido pronta!\")\n",
        "print(\"üí° Esempio: quick_test_config(window_size=30, model_type='dense')\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IDS Pipeline Audit: Preprocessing avanzato e Training\n",
        "\n",
        "Questa sezione documenta dettagliatamente le scelte progettuali e operative per:\n",
        "- **Flow reassembly** e **sessionizzazione** (timeout configurabile)\n",
        "- **Finestre N/T** attorno al primo evento malevolo per `Flow_ID`\n",
        "- **Label propagation** con modalit√† configurabile (any/majority/probabilistica/smoothing)\n",
        "- **Noise handling** tramite majority/temporal smoothing\n",
        "- **Dataset balancing** a livello di flusso (undersampling/SMOTE)\n",
        "- **Output** per modelli sequenziali e MLP aggregato\n",
        "- **Supporto training** con MLP 4 layer e logging delle loss\n",
        "\n",
        "Tutte le funzioni sono eseguibili da **comandi**, per favorire riproducibilit√† e tracciabilit√†.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Struttura della pipeline (overview)\n",
        "\n",
        "- **Input**: CSV con campi `Flow_ID`, `Timestamp`, `Label` (es. `BENIGN` o altro)\n",
        "- **Reassembly & Sessionizzazione**: raggruppa per `Flow_ID` e separa in `Session_ID` con timeout (default 60s)\n",
        "- **Noise filtering**: smoothing/majority sulle etichette temporali\n",
        "- **Finestra N/T**: per ogni `Session_ID`, crea una finestra centrata sul primo evento malevolo con `N` secondi prima e `T` dopo\n",
        "- **Aggregazione**: in bin temporali (e.g., 5s) per sequenze; oppure aggregazione globale per MLP\n",
        "- **Label propagation**: se la finestra contiene almeno un malevolo (o secondo strategia), etichetta la finestra come malevola\n",
        "- **Bilanciamento**: seleziona tutte le sessioni malevole e un ugual numero di `BENIGN` (undersample) o usa SMOTE\n",
        "- **Output**: salva `X.npy` e `y.npy` o shard per-epoca per training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parametri configurabili (config.py)\n",
        "\n",
        "- **Dataset**:\n",
        "  - `DATA_CONFIG.dataset_path`, `timestamp_column`, `flow_id_column`, `target_column`, `benign_label`, `timestamp_format`\n",
        "- **Sessionizzazione**:\n",
        "  - `PREPROCESSING_CONFIG.session_timeout_seconds`\n",
        "- **Finestre**:\n",
        "  - `flow_window_strategy` (e.g., `first_malicious_context`)\n",
        "  - `window_before_first_malicious_s` (N), `window_after_first_malicious_s` (T)\n",
        "  - `time_bin_seconds` (granularit√† per sequenze)\n",
        "- **Label propagation / Noise**:\n",
        "  - `label_propagation.mode` (`any`/`majority`/`probabilistic`/`smoothing`)\n",
        "  - `prob_threshold`, `smoothing_alpha`\n",
        "  - `noise_filter.enabled`, `noise_filter.method`, `noise_filter.window`, `noise_filter.threshold`\n",
        "- **Bilanciamento a flusso**:\n",
        "  - `flow_balance.enabled`, `flow_balance.method` (`undersample`/`smote`/`none`), `flow_balance.ratio`\n",
        "- **Output**:\n",
        "  - `output_mode` (`sequence`/`mlp_aggregated`), `aggregation_stats`\n",
        "- **Training**:\n",
        "  - `model_type`, `mlp_hidden_layers`, `dropout_rate`, `hyperparameters.epochs/batch_size/learning_rate`, `max_epochs`\n",
        "  - `preprocess_per_epoch`, `flows_per_epoch`, `epoch_selection_mode`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Esecuzione preprocessing (solo comandi)\n",
        "\n",
        "1) Installare dipendenze\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "2) Verifica/adegua `config.py` (path dataset, colonne, parametri)\n",
        "\n",
        "3) Esegui preprocessing completo\n",
        "```bash\n",
        "python -m preprocessing.process | cat\n",
        "```\n",
        "\n",
        "- Output: `models/preprocessed/X.npy`, `models/preprocessed/y.npy`\n",
        "- Se `preprocess_per_epoch=True`: `models/preprocessed/X_epoch_XXX.npy`, `y_epoch_XXX.npy`\n",
        "\n",
        "4) Parametri chiave da ricordare\n",
        "- `session_timeout_seconds`, `window_before_first_malicious_s`, `window_after_first_malicious_s`, `time_bin_seconds`\n",
        "- `label_propagation.mode`, `noise_filter.*`\n",
        "- `flow_balance.method`, `flow_balance.ratio`\n",
        "- `output_mode` (`sequence` vs `mlp_aggregated`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Esecuzione training (solo comandi)\n",
        "\n",
        "1) Seleziona modello\n",
        "- `TRAINING_CONFIG.model_type`: `gru`, `lstm`, `dense`\n",
        "- Se `X.npy` √® 2D (MLP aggregato), la CLI forza `dense`\n",
        "\n",
        "2) Avvia training\n",
        "```bash\n",
        "python -m training.train | cat\n",
        "```\n",
        "\n",
        "- Salvataggi:\n",
        "  - Modello: `models/best_model.keras`\n",
        "  - Log configurazioni: `models/training_log.json`\n",
        "  - Storico loss/accuracy per epoca: `models/training_history.json`\n",
        "\n",
        "3) Epoche e loss\n",
        "- Fino a `max_epochs=30`\n",
        "- Tracciamento `loss`/`val_loss` per analisi convergenza\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Considerazioni progettuali e trade-off\n",
        "\n",
        "- **Sessionizzazione**: separa conversazioni distinte sullo stesso `Flow_ID` per evitare leakage temporale.\n",
        "- **N/T finestra**: simula detection realistiche (contesto pre-attacco e persistenza post-detection). Importante per ridurre falsi positivi.\n",
        "- **Label propagation**: modalit√† `any` garantisce recall, `majority` bilancia precision/recall, `probabilistic` consente soglia adattiva, `smoothing` robusto a label rumorose.\n",
        "- **Bilanciamento a flusso**: evitare che pochi attacchi dominino; `undersample` √® stabile, `SMOTE` richiede feature aggregate per sessione.\n",
        "- **Output**: `sequence` utile per RNN/GRU/LSTM; `mlp_aggregated` per MLP rapido con statistiche robuste.\n",
        "- **Robustezza a label imperfette**: smoothing temporale e majority voting mitigano errori di annotazione.\n",
        "- **Per-epoca**: shard per allenare in sequenza o parallelo sottinsiemi di flussi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep dive: Sessionizzazione e reassembly\n",
        "\n",
        "- Obiettivo: ricostruire conversazioni coerenti gestendo discontinuit√† temporali sullo stesso `Flow_ID`.\n",
        "- Algoritmo:\n",
        "  - Ordina per `Flow_ID` e `Timestamp`.\n",
        "  - Avvia `Session_Index=0` al primo record; incrementa l'indice quando l'intervallo tra record successivi supera `session_timeout_seconds`.\n",
        "  - Costruisci `Session_ID = Flow_ID#Session_Index`.\n",
        "- Propriet√†:\n",
        "  - Complessit√† lineare O(N) dopo ordinamento.\n",
        "  - Resiliente a timestamp mancanti: avvia una nuova sessione quando il timestamp non √® valido.\n",
        "- Rischi/Pitfall:\n",
        "  - Timeout troppo corto frammenta sessioni; troppo lungo fonde conversazioni distinte.\n",
        "  - Assicurare la normalizzazione del fuso e il parsing coerente dei timestamp (`timestamp_format` se necessario).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep dive: Noise handling (majority & temporal smoothing)\n",
        "\n",
        "- Majority window: media mobile della variabile binaria `is_malicious`; soglia `threshold` decide la classe.\n",
        "- Temporal smoothing (EMA): `s_t = alpha * x_t + (1 - alpha) * s_{t-1}` con `alpha` in `(0,1]`.\n",
        "  - `alpha` alto reagisce pi√π velocemente ma √® pi√π sensibile al rumore; `alpha` basso √® pi√π stabile.\n",
        "- Applicazione per sessione: evita bleed-over tra flussi diversi.\n",
        "- Effetti:\n",
        "  - Riduce flip sporadici delle etichette.\n",
        "  - Utile con dataset reali con labeling non perfetto o delay di annotazione.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep dive: Label propagation (any/majority/probabilistica/smoothing)\n",
        "\n",
        "- any: recall massimo; finestra = malevola se presente QUALSIASI evento malevolo.\n",
        "- majority: trade-off; richiede >50% eventi malevoli nella finestra.\n",
        "- probabilistica: imposta soglia `prob_threshold` (es. 0.3, 0.5, 0.7) sulla frazione di eventi malevoli.\n",
        "- smoothing: usa output filtrato (EMA/majority) e poi majority \"soft\" per la finestra.\n",
        "- Scelta operativa:\n",
        "  - In ambienti ad alto rischio ‚Üí preferire `any` o soglia bassa.\n",
        "  - Per ridurre falsi positivi in produzione ‚Üí `majority` o `smoothing`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep dive: Finestre N/T e binning temporale\n",
        "\n",
        "- Strategia `first_malicious_context`:\n",
        "  - Trova il primo timestamp malevolo nella sessione.\n",
        "  - Crea una finestra da `N` secondi prima a `T` secondi dopo.\n",
        "- Binning:\n",
        "  - Discretizza la finestra in intervalli di `time_bin_seconds` (es. 5s).\n",
        "  - Aggrega le feature per bin per costruire sequenze temporali regolari.\n",
        "- Mancanza di eventi in un bin:\n",
        "  - Il bin risulta zero o con statistiche neutre; il padding finale uniforma le lunghezze.\n",
        "- Allineamento:\n",
        "  - Base temporale = minimo timestamp della finestra per robustezza.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep dive: Aggregazioni statistiche e feature engineering\n",
        "\n",
        "- Statistiche per bin/finestra: `sum`, `mean`, `std`, `min`, `max`.\n",
        "- Perch√©:\n",
        "  - `sum` e `mean` catturano intensit√† media; `std` quantifica variabilit√†; `min/max` estremi utili per burst.\n",
        "- Estensioni possibili (future):\n",
        "  - Entropia di porte/destinazioni IP per finestra; tassi di errore; inter-arrival variance robusta.\n",
        "  - Indicatori di burst (peak-to-average ratio), code di coda (kurtosi) per anomalia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bilanciamento a livello di flusso\n",
        "\n",
        "- Definizione di finestra/Session come unit√† di campionamento.\n",
        "- `undersample`: seleziona tutte le sessioni malevole + sottoinsieme di `BENIGN` fino a parit√† (configurabile via `ratio`).\n",
        "- `SMOTE`: richiede rappresentazione tabellare per sessione (aggregate), poi genera campioni sintetici nello spazio feature.\n",
        "- Considerazioni:\n",
        "  - `SMOTE` su feature aggregate preserva statistiche di finestra ma non la cronologia di pacchetti.\n",
        "  - Alternativa avanzata: ADASYN o SMOTE-NC per misti cat/num.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comandi utili di audit e ispezione artefatti\n",
        "\n",
        "- Verifica artefatti preprocessing\n",
        "```bash\n",
        "ls -lh models/preprocessed | cat\n",
        "```\n",
        "\n",
        "- Ispeziona dimensioni dei file NPY (solo comandi)\n",
        "```bash\n",
        "for f in models/preprocessed/*.npy; do python -c \"import numpy as np, sys; a=np.load(sys.argv[1], allow_pickle=False); print(sys.argv[1], a.shape)\" \"$f\"; done | cat\n",
        "```\n",
        "\n",
        "- Installa jq (se mancante) e visualizza log/history\n",
        "```bash\n",
        "sudo apt-get update && sudo apt-get install -y jq\n",
        "jq '.' models/training_log.json | head -n 50 | cat\n",
        "jq '.' models/training_history.json | head -n 80 | cat\n",
        "```\n",
        "\n",
        "- Pulizia artefatti\n",
        "```bash\n",
        "rm -f models/preprocessed/*.npy models/best_model.keras models/training_*.json\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training per-epoca con shard\n",
        "\n",
        "Se `TRAINING_CONFIG.preprocess_per_epoch=True`, il preprocessing salva shard `X_epoch_XXX.npy` e `y_epoch_XXX.npy`.\n",
        "\n",
        "- Esempio di training sequenziale per shard (solo comandi)\n",
        "```bash\n",
        "for xp in models/preprocessed/X_epoch_*.npy; do \n",
        "  yp=${xp/X_epoch_/y_epoch_}\n",
        "  echo \"== Training su $xp ==\"\n",
        "  # sostituisci i file full con shard correnti\n",
        "  cp \"$xp\" models/preprocessed/X.npy\n",
        "  cp \"$yp\" models/preprocessed/y.npy\n",
        "  python -m training.train | cat\n",
        "done\n",
        "```\n",
        "\n",
        "- Esempio di ispezione prestazioni cumulative (placeholder, solo aggregazione file)\n",
        "```bash\n",
        "cat models/training_log.json | wc -l\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "dl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN-IDS Audit Notebook (CSE-CIC-IDS2018)\n",
        "\n",
        "Questo notebook √® pensato per essere auditabile: ogni scelta √® documentata file-per-file e riga-per-riga dove rilevante. Include:\n",
        "- Setup ambiente e dati\n",
        "- Pipeline dati riproducibile\n",
        "- Training recipe ottimizzata per tabulari (GRU per finestre temporali)\n",
        "- Metriche e calibrazione\n",
        "- Smoke test e test completo (finestre: 5s, 1m, 5m; LR grid)\n",
        "\n",
        "Note: il codice vive nel repository; il notebook chiama le funzioni senza duplicazioni di logica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup ambiente\n",
        "\n",
        "Requisiti minimi:\n",
        "- Python 3.10+\n",
        "- pacchetti: pandas, numpy, scikit-learn, tensorflow, matplotlib, seaborn, tqdm\n",
        "\n",
        "In Colab eseguire le celle seguenti; in locale assicurarsi che `pip install -r requirements.txt` sia stato eseguito.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Se sei in Colab, decommenta le righe seguenti\n",
        "!git clone https://github.com/devedale/snn-ids.git\n",
        "%cd snn-ids\n",
        "!pip install -q pandas numpy scikit-learn tensorflow matplotlib seaborn tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Setup dati\n",
        "Scarica i CSV CSE-CIC-IDS2018 nelle cartelle gi√† attese da `config.py` (`data/cicids/2018`). In Colab puoi caricare dal tuo Drive o usare Kaggle API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd snn-ids\n",
        "!curl -L -o ./data/cicids2018.zip  https://www.kaggle.com/api/v1/datasets/download/edoardodalesio/intrusion-detection-evaluation-dataset-cic-ids2018\n",
        "#!unzip -o ./data/cicids2018.zip  \"Tuesday-20-02-2018.csv\"   \"Wednesday-21-02-2018.csv\"  \"Thursday-22-02-2018.csv\" \"Friday-23-02-2018.csv\" -d ./data\n",
        "!unzip -o ./data/cicids2018.zip -d ./data\n",
        "# Importa librerie del progetto\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from config import DATA_CONFIG, PREPROCESSING_CONFIG, TRAINING_CONFIG, BENCHMARK_CONFIG\n",
        "from preprocessing.process import preprocess_pipeline\n",
        "from training.train import train_model\n",
        "from evaluation.metrics import evaluate_model_comprehensive\n",
        "\n",
        "print(\"Config dataset:\", DATA_CONFIG[\"dataset_path\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Smoke test (GRU)\n",
        "Esegue pipeline ridotta per verificare fine-to-end: bilanciamento security, IP‚Üíottetti, finestre, training GRU (K-Fold), valutazione con PNG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 benchmark.py --smoke-test --data-path data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Test completo (GRU) con finestre 5s, 1m, 5m e grid LR\n",
        "In questo test variamo:\n",
        "- finestre temporali: `window_size` e `step` coerenti con risoluzioni 5s, 1m, 5m\n",
        "- learning rate: `[1e-3, 5e-4, 1e-4]`\n",
        "- epoche moderate per tempi ragionevoli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "results = []\n",
        "base_prep = deepcopy(PREPROCESSING_CONFIG)\n",
        "base_train = deepcopy(TRAINING_CONFIG)\n",
        "\n",
        "# Grid finestre (timesteps) e learning rate\n",
        "window_configs = [\n",
        "    {\"name\": \"5s\", \"window_size\": 10, \"step\": 5},\n",
        "    {\"name\": \"1m\", \"window_size\": 60//6, \"step\": 10},  # es: 10 step \n",
        "    {\"name\": \"5m\", \"window_size\": 50, \"step\": 10},\n",
        "]\n",
        "lr_grid = [1e-3, 5e-4, 1e-4]\n",
        "\n",
        "for wc in window_configs:\n",
        "    PREPROCESSING_CONFIG['use_time_windows'] = True\n",
        "    PREPROCESSING_CONFIG['window_size'] = wc['window_size']\n",
        "    PREPROCESSING_CONFIG['step'] = wc['step']\n",
        "    \n",
        "    for lr in lr_grid:\n",
        "        TRAINING_CONFIG['model_type'] = 'gru'\n",
        "        TRAINING_CONFIG['hyperparameters']['epochs'] = [5]\n",
        "        TRAINING_CONFIG['hyperparameters']['batch_size'] = [64]\n",
        "        TRAINING_CONFIG['hyperparameters']['learning_rate'] = [lr]\n",
        "        \n",
        "        print(f\"\\n=== Config: {wc['name']} | lr={lr} ===\")\n",
        "        X, y, le = preprocess_pipeline()\n",
        "        model, log, path = train_model(X, y, model_type='gru')\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        rep = evaluate_model_comprehensive(model, X_te, y_te, le.classes_.tolist(), output_dir=f'notebook_eval/{wc[\"name\"]}_lr{lr}')\n",
        "        results.append({'window': wc['name'], 'lr': lr, 'accuracy': rep['basic_metrics']['accuracy']})\n",
        "\n",
        "# Ripristina config\n",
        "PREPROCESSING_CONFIG.update(base_prep)\n",
        "TRAINING_CONFIG.update(base_train)\n",
        "\n",
        "pd.DataFrame(results).sort_values('accuracy', ascending=False).head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Riproducibilit√†\n",
        "Impostiamo i seed per rendere i risultati ripetibili (entro i limiti dell'hardware).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print('Seed impostato:', SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Audit dati e feature\n",
        "Controlliamo distribuzione classi, percentuali, e presenza di attacchi rilevanti nel sample selezionato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def audit_distribution(y, label_encoder):\n",
        "    counts = Counter(y)\n",
        "    classes = label_encoder.classes_.tolist()\n",
        "    dist = {classes[i]: int(counts.get(i, 0)) for i in range(len(classes))}\n",
        "    total = sum(dist.values())\n",
        "    df = pd.DataFrame({\n",
        "        'classe': list(dist.keys()),\n",
        "        'conteggio': list(dist.values())\n",
        "    }).sort_values('conteggio', ascending=False)\n",
        "    df['percentuale'] = (df['conteggio'] / total * 100).round(2)\n",
        "    return df\n",
        "\n",
        "# Esempio live (riutilizza X,y,label_encoder se esistono)\n",
        "try:\n",
        "    audit_distribution(y, label_encoder)\n",
        "except Exception as e:\n",
        "    print('Esegui prima il smoke test per generare X,y,label_encoder')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Metriche e calibrazione\n",
        "Oltre alle metriche standard, aggiungiamo ECE (Expected Calibration Error) per valutare la calibrazione delle probabilit√†.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def expected_calibration_error(y_true, y_proba, n_bins=10):\n",
        "    # binning su max probability\n",
        "    confidences = y_proba.max(axis=1)\n",
        "    predictions = y_proba.argmax(axis=1)\n",
        "    accuracies = (predictions == y_true).astype(float)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
        "        if mask.any():\n",
        "            avg_conf = confidences[mask].mean()\n",
        "            avg_acc = accuracies[mask].mean()\n",
        "            ece += np.abs(avg_acc - avg_conf) * mask.mean()\n",
        "    return float(ece)\n",
        "\n",
        "# Esempio: usa il modello dallo smoke test, se disponibile\n",
        "try:\n",
        "    y_proba = model.predict(X_te, verbose=0)\n",
        "    print('ECE:', expected_calibration_error(y_te, y_proba, n_bins=15))\n",
        "except Exception as e:\n",
        "    print('Esegui prima smoke test e valutazione per avere y_te e y_proba')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Documentazione file-per-file\n",
        "In questa sezione spieghiamo le scelte implementative nei file chiave: `preprocessing/process.py`, `training/train.py`, `evaluation/metrics.py`, `benchmark.py` e `config.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect, textwrap\n",
        "import preprocessing.process as P\n",
        "import training.train as T\n",
        "import evaluation.metrics as E\n",
        "import benchmark as B\n",
        "import config as C\n",
        "\n",
        "def show_source(obj, start=None, end=None):\n",
        "    src = inspect.getsource(obj)\n",
        "    if start or end:\n",
        "        lines = src.splitlines()\n",
        "        src = \"\\n\".join(lines[start:end])\n",
        "    print(textwrap.dedent(src))\n",
        "\n",
        "print('--- config.py (sezioni principali) ---')\n",
        "print('DATA_CONFIG:'); print(C.DATA_CONFIG)\n",
        "print('\\nPREPROCESSING_CONFIG:'); print(C.PREPROCESSING_CONFIG)\n",
        "print('\\nTRAINING_CONFIG:'); print(C.TRAINING_CONFIG)\n",
        "\n",
        "print('\\n--- preprocessing.process: load_and_balance_dataset ---')\n",
        "show_source(P.load_and_balance_dataset)\n",
        "print('\\n--- preprocessing.process: preprocess_pipeline ---')\n",
        "show_source(P.preprocess_pipeline)\n",
        "\n",
        "print('\\n--- training.train: _train_k_fold ---')\n",
        "show_source(T._train_k_fold)\n",
        "print('\\n--- training.train: _train_split ---')\n",
        "show_source(T._train_split)\n",
        "\n",
        "print('\\n--- evaluation.metrics: evaluate_model_comprehensive ---')\n",
        "show_source(E.evaluate_model_comprehensive)\n",
        "\n",
        "print('\\n--- benchmark.SNNIDSBenchmark (run_smoke_test) ---')\n",
        "show_source(B.SNNIDSBenchmark.run_smoke_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note progettuali\n",
        "- Zero hard-code: tutte le scelte sono in `config.py`; il notebook applica override solo per esperimenti.\n",
        "- Pipeline riproducibile: sampling e bilanciamento documentati; seed fissati.\n",
        "- Training recipe tabulari: GRU su finestre 3D, scaling per-fold, StratifiedKFold.\n",
        "- Metriche e PNG: confusion matrix dettagliata, cybersecurity, ROC, accuracy per classe, ECE.\n",
        "- Notebook auditabile: usa `inspect` per mostrare il codice sorgente eseguito.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Limitazioni\n",
        "- I risultati su classi rare vanno interpretati con cautela; forniamo sempre breakdown per‚Äëclasse.\n",
        "- La calibrazione (ECE) √® informativa ma non esaustiva.\n",
        "- Il bilanciamento ‚Äúsecurity‚Äù riduce bias ma non sostituisce protocolli di acquisizione realistici.\n",
        "- Evitiamo leakage scalando per‚Äëfold; ulteriori audit sono comunque consigliati in ambienti operativi.\n",
        "- Per produzione sono necessarie valutazioni cost‚Äësensitive e monitoraggio del drift.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
