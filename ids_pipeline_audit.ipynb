{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN-IDS Audit Notebook (CSE-CIC-IDS2018)\n",
        "\n",
        "Questo notebook è pensato per essere auditabile: ogni scelta è documentata file-per-file e riga-per-riga dove rilevante. Include:\n",
        "- Setup ambiente e dati\n",
        "- Pipeline dati riproducibile\n",
        "- Training recipe ottimizzata per tabulari (GRU per finestre temporali)\n",
        "- Metriche e calibrazione\n",
        "- Smoke test e test completo (finestre: 5s, 1m, 5m; LR grid)\n",
        "\n",
        "Note: il codice vive nel repository; il notebook chiama le funzioni senza duplicazioni di logica.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup ambiente\n",
        "\n",
        "Requisiti minimi:\n",
        "- Python 3.10+\n",
        "- pacchetti: pandas, numpy, scikit-learn, tensorflow, matplotlib, seaborn, tqdm\n",
        "\n",
        "In Colab eseguire le celle seguenti; in locale assicurarsi che `pip install -r requirements.txt` sia stato eseguito.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Se sei in Colab, decommenta le righe seguenti\n",
        "!git clone --single-branch --branch fix-branch https://github.com/devedale/snn-ids.git\n",
        "#!git clone https://github.com/devedale/snn-ids.git\n",
        "%cd snn-ids\n",
        "!pip install -q pandas numpy scikit-learn tensorflow matplotlib seaborn tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Setup dati\n",
        "Scarica i CSV CSE-CIC-IDS2018 nelle cartelle già attese da `config.py` (`data/cicids/2018`). In Colab puoi caricare dal tuo Drive o usare Kaggle API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Uncomment to download full starting dataset and comment others\n",
        "#!curl -L -o ./data/cicids2018.zip  https://www.kaggle.com/api/v1/datasets/download/edoardodalesio/intrusion-detection-evaluation-dataset-cic-ids2018\n",
        "#!unzip -o ./data/cicids2018.zip  \"Tuesday-20-02-2018.csv\"   \"Wednesday-21-02-2018.csv\"  \"Thursday-22-02-2018.csv\" \"Friday-23-02-2018.csv\" -d ./data # or #!unzip -o ./data/cicids2018.zip -d ./data\n",
        "\n",
        "# Importa librerie del progetto\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from config import DATA_CONFIG, PREPROCESSING_CONFIG, TRAINING_CONFIG, BENCHMARK_CONFIG\n",
        "from preprocessing.process import preprocess_pipeline\n",
        "from training.train import train_model\n",
        "from evaluation.metrics import evaluate_model_comprehensive\n",
        "\n",
        "print(\"Config dataset:\", DATA_CONFIG[\"dataset_path\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Uncomment to use preprocessed cache and comment others\n",
        "#!mkdir preprocessed_cache\n",
        "#!curl -L -o ./preprocessed_cache/preprocessed_cache.zip  https://www.kaggle.com/api/v1/datasets/download/edoardodalesio/cic-ids-2018-benign-vs-attack\n",
        "#!unzip -o ./preprocessed_cache/preprocessed_cache.zip -d ./preprocessed_cache\n",
        "#!touch data/Friday-02-03-2018.csv data/Friday-16-02-2018.csv data/Friday-23-02-2018.csv data/Thursday-01-03-2018.csv data/Thursday-15-02-2018.csv data/Thursday-22-02-2018.csv data/Tuesday-20-02-2018.csv data/Wednesday-14-02-2018.csv data/Wednesday-21-02-2018.csv data/Wednesday-28-02-2018.csv\n",
        "\n",
        "#Uncomment to use model cache and comment others\n",
        "!mkdir preprocessed_cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python3 benchmark.py --full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esempi di utilizzo:\n",
        "\n",
        "  # 1. Eseguire un test rapido (smoke test) per verificare che tutto funzioni\n",
        "  python3 benchmark.py --smoke-test\n",
        "\n",
        "  # 2. Eseguire un singolo test con un modello specifico e dimensione del campione\n",
        "  python3 benchmark.py --model gru --sample-size 20000\n",
        "\n",
        "  # 3. Eseguire il benchmark completo su tutti i modelli e iperparametri di default\n",
        "  python3 benchmark.py --full\n",
        "\n",
        "  # 4. Eseguire il benchmark completo con una dimensione del campione personalizzata\n",
        "  python3 benchmark.py --full --sample-size 50000\n",
        "\n",
        "  # 5. Eseguire un singolo test specificando iperparametri custom (nota: devono essere nel formato atteso dal modulo di training)\n",
        "  python3 benchmark.py --model lstm --epochs 15 --batch-size 128 --learning-rate 0.0005\n",
        "        '''\n",
        "    )\n",
        "    \n",
        "    # Argomenti principali per la selezione della modalità\n",
        "    parser.add_argument('--smoke-test', action='store_true', help='Esegue uno smoke test veloce e leggero.')\n",
        "    parser.add_argument('--full', action='store_true', help='Esegue il benchmark completo su più modelli e iperparametri.')\n",
        "    \n",
        "    # Argomenti per la configurazione di base\n",
        "    parser.add_argument('--sample-size', type=int, help='Numero totale di campioni da utilizzare (BENIGN + ATTACK).')\n",
        "    parser.add_argument('--data-path', type=str, help='Path alla directory contenente i file CSV del dataset.')\n",
        "    parser.add_argument('--output-dir', type=str, default='benchmark_results', help='Directory per salvare i risultati.')\n",
        "\n",
        "    # Argomenti per la configurazione del modello (usati in test singoli o come override)\n",
        "    parser.add_argument('--model', choices=['dense', 'gru', 'lstm'], help='Tipo di modello da testare in un singolo run.')\n",
        "    parser.add_argument('--epochs', type=int, help=\"Override del numero di epoche per il training (es. 10).\")\n",
        "    parser.add_argument('--batch-size', type=int, help=\"Override della batch size per il training (es. 64).\")\n",
        "    parser.add_argument('--learning-rate', type=float, help=\"Override del learning rate (es. 0.001).\")\n",
        "    \n",
        "    args = parser.parse_args()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Smoke test (GRU)\n",
        "Esegue pipeline ridotta per verificare fine-to-end: bilanciamento security, IP→ottetti, finestre, training GRU (K-Fold), valutazione con PNG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Override per test rapido\n",
        "PREPROCESSING_CONFIG['sample_size'] = 3000\n",
        "TRAINING_CONFIG['model_type'] = 'gru'\n",
        "TRAINING_CONFIG['hyperparameters']['epochs'] = [2]\n",
        "TRAINING_CONFIG['hyperparameters']['batch_size'] = [32]\n",
        "\n",
        "X, y, label_encoder = preprocess_pipeline()\n",
        "model, log, model_path = train_model(X, y, model_type='gru')\n",
        "\n",
        "# Valutazione rapida\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "report = evaluate_model_comprehensive(model, X_te, y_te, class_names=label_encoder.classes_.tolist(), output_dir='notebook_eval/smoke')\n",
        "report['basic_metrics']['accuracy']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Test completo (GRU) con finestre 5s, 1m, 5m e grid LR\n",
        "In questo test variamo:\n",
        "- finestre temporali: `window_size` e `step` coerenti con risoluzioni 5s, 1m, 5m\n",
        "- learning rate: `[1e-3, 5e-4, 1e-4]`\n",
        "- epoche moderate per tempi ragionevoli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "results = []\n",
        "base_prep = deepcopy(PREPROCESSING_CONFIG)\n",
        "base_train = deepcopy(TRAINING_CONFIG)\n",
        "\n",
        "# Grid finestre (timesteps) e learning rate\n",
        "window_configs = [\n",
        "    {\"name\": \"5s\", \"window_size\": 10, \"step\": 5},\n",
        "    {\"name\": \"1m\", \"window_size\": 60//6, \"step\": 10},  # es: 10 step \n",
        "    {\"name\": \"5m\", \"window_size\": 50, \"step\": 10},\n",
        "]\n",
        "lr_grid = [1e-3, 5e-4, 1e-4]\n",
        "\n",
        "for wc in window_configs:\n",
        "    PREPROCESSING_CONFIG['use_time_windows'] = True\n",
        "    PREPROCESSING_CONFIG['window_size'] = wc['window_size']\n",
        "    PREPROCESSING_CONFIG['step'] = wc['step']\n",
        "    \n",
        "    for lr in lr_grid:\n",
        "        TRAINING_CONFIG['model_type'] = 'gru'\n",
        "        TRAINING_CONFIG['hyperparameters']['epochs'] = [5]\n",
        "        TRAINING_CONFIG['hyperparameters']['batch_size'] = [64]\n",
        "        TRAINING_CONFIG['hyperparameters']['learning_rate'] = [lr]\n",
        "        \n",
        "        print(f\"\\n=== Config: {wc['name']} | lr={lr} ===\")\n",
        "        X, y, le = preprocess_pipeline()\n",
        "        model, log, path = train_model(X, y, model_type='gru')\n",
        "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "        rep = evaluate_model_comprehensive(model, X_te, y_te, le.classes_.tolist(), output_dir=f'notebook_eval/{wc[\"name\"]}_lr{lr}')\n",
        "        results.append({'window': wc['name'], 'lr': lr, 'accuracy': rep['basic_metrics']['accuracy']})\n",
        "\n",
        "# Ripristina config\n",
        "PREPROCESSING_CONFIG.update(base_prep)\n",
        "TRAINING_CONFIG.update(base_train)\n",
        "\n",
        "pd.DataFrame(results).sort_values('accuracy', ascending=False).head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Riproducibilità\n",
        "Impostiamo i seed per rendere i risultati ripetibili (entro i limiti dell'hardware).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "print('Seed impostato:', SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Audit dati e feature\n",
        "Controlliamo distribuzione classi, percentuali, e presenza di attacchi rilevanti nel sample selezionato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def audit_distribution(y, label_encoder):\n",
        "    counts = Counter(y)\n",
        "    classes = label_encoder.classes_.tolist()\n",
        "    dist = {classes[i]: int(counts.get(i, 0)) for i in range(len(classes))}\n",
        "    total = sum(dist.values())\n",
        "    df = pd.DataFrame({\n",
        "        'classe': list(dist.keys()),\n",
        "        'conteggio': list(dist.values())\n",
        "    }).sort_values('conteggio', ascending=False)\n",
        "    df['percentuale'] = (df['conteggio'] / total * 100).round(2)\n",
        "    return df\n",
        "\n",
        "# Esempio live (riutilizza X,y,label_encoder se esistono)\n",
        "try:\n",
        "    audit_distribution(y, label_encoder)\n",
        "except Exception as e:\n",
        "    print('Esegui prima il smoke test per generare X,y,label_encoder')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Metriche e calibrazione\n",
        "Oltre alle metriche standard, aggiungiamo ECE (Expected Calibration Error) per valutare la calibrazione delle probabilità.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def expected_calibration_error(y_true, y_proba, n_bins=10):\n",
        "    # binning su max probability\n",
        "    confidences = y_proba.max(axis=1)\n",
        "    predictions = y_proba.argmax(axis=1)\n",
        "    accuracies = (predictions == y_true).astype(float)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        mask = (confidences > bins[i]) & (confidences <= bins[i+1])\n",
        "        if mask.any():\n",
        "            avg_conf = confidences[mask].mean()\n",
        "            avg_acc = accuracies[mask].mean()\n",
        "            ece += np.abs(avg_acc - avg_conf) * mask.mean()\n",
        "    return float(ece)\n",
        "\n",
        "# Esempio: usa il modello dallo smoke test, se disponibile\n",
        "try:\n",
        "    y_proba = model.predict(X_te, verbose=0)\n",
        "    print('ECE:', expected_calibration_error(y_te, y_proba, n_bins=15))\n",
        "except Exception as e:\n",
        "    print('Esegui prima smoke test e valutazione per avere y_te e y_proba')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Documentazione file-per-file\n",
        "In questa sezione spieghiamo le scelte implementative nei file chiave: `preprocessing/process.py`, `training/train.py`, `evaluation/metrics.py`, `benchmark.py` e `config.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import inspect, textwrap\n",
        "import preprocessing.process as P\n",
        "import training.train as T\n",
        "import evaluation.metrics as E\n",
        "import benchmark as B\n",
        "import config as C\n",
        "\n",
        "def show_source(obj, start=None, end=None):\n",
        "    src = inspect.getsource(obj)\n",
        "    if start or end:\n",
        "        lines = src.splitlines()\n",
        "        src = \"\\n\".join(lines[start:end])\n",
        "    print(textwrap.dedent(src))\n",
        "\n",
        "print('--- config.py (sezioni principali) ---')\n",
        "print('DATA_CONFIG:'); print(C.DATA_CONFIG)\n",
        "print('\\nPREPROCESSING_CONFIG:'); print(C.PREPROCESSING_CONFIG)\n",
        "print('\\nTRAINING_CONFIG:'); print(C.TRAINING_CONFIG)\n",
        "\n",
        "print('\\n--- preprocessing.process: load_and_balance_dataset ---')\n",
        "show_source(P.load_and_balance_dataset)\n",
        "print('\\n--- preprocessing.process: preprocess_pipeline ---')\n",
        "show_source(P.preprocess_pipeline)\n",
        "\n",
        "print('\\n--- training.train: _train_k_fold ---')\n",
        "show_source(T._train_k_fold)\n",
        "print('\\n--- training.train: _train_split ---')\n",
        "show_source(T._train_split)\n",
        "\n",
        "print('\\n--- evaluation.metrics: evaluate_model_comprehensive ---')\n",
        "show_source(E.evaluate_model_comprehensive)\n",
        "\n",
        "print('\\n--- benchmark.SNNIDSBenchmark (run_smoke_test) ---')\n",
        "show_source(B.SNNIDSBenchmark.run_smoke_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note progettuali\n",
        "- Zero hard-code: tutte le scelte sono in `config.py`; il notebook applica override solo per esperimenti.\n",
        "- Pipeline riproducibile: sampling e bilanciamento documentati; seed fissati.\n",
        "- Training recipe tabulari: GRU su finestre 3D, scaling per-fold, StratifiedKFold.\n",
        "- Metriche e PNG: confusion matrix dettagliata, cybersecurity, ROC, accuracy per classe, ECE.\n",
        "- Notebook auditabile: usa `inspect` per mostrare il codice sorgente eseguito.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Limitazioni\n",
        "- I risultati su classi rare vanno interpretati con cautela; forniamo sempre breakdown per‑classe.\n",
        "- La calibrazione (ECE) è informativa ma non esaustiva.\n",
        "- Il bilanciamento “security” riduce bias ma non sostituisce protocolli di acquisizione realistici.\n",
        "- Evitiamo leakage scalando per‑fold; ulteriori audit sono comunque consigliati in ambienti operativi.\n",
        "- Per produzione sono necessarie valutazioni cost‑sensitive e monitoraggio del drift.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
