{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# SNN (Nengo/NengoDL) su NSL-KDD ‚Äî Colab\\n\\nNotebook end-to-end: clona la repo, installa requisiti, scarica NSL-KDD, preprocessing, rete Nengo con neuroni LIF, training con NengoDL, valutazione e salvataggio parametri.\\n\\n- Repo: https://github.com/devedale/snn-ids\\n- Framework: Nengo + NengoDL (TensorFlow backend)\\n- Codifica: rate coding (ripetizione su T step)\\n- Task: binaria (anomalo vs normale)\\n\\n**Nota sui conflitti pip**: I warning di dependency conflicts sono normali in Colab. NengoDL richiede TensorFlow 2.12.1 + tf-keras 2.15.0 che confliggono con alcuni pacchetti preinstallati, ma il notebook funziona comunque."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Cleanup ambiente + install compatibili (NengoDL) e riavvio runtime\\n!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\\n%cd /content/snn-ids\\n\\n# Rimuove pacchetti preinstallati che confliggono con TF/Keras 2.x\\n!pip -q uninstall -y jax jaxlib tensorflow-text tensorflow-decision-forests tensorflow-hub \\\\\\n  orbax-checkpoint chex optax flax dopamine-rl tensorstore || true\\n!pip -q uninstall -y tensorflow tensorflow-cpu tensorflow-gpu keras-hub opencv-python opencv-contrib-python opencv-python-headless || true\\n!pip -q uninstall -y keras keras-nightly tf-keras keras-preprocessing keras-vis || true\\n!pip -q uninstall -y albucore albumentations ydf grpcio-status typeguard torch torchvision torchaudio fastai sentence-transformers peft timm accelerate || true\\n\\n# Aggiorna toolchain e installa requisiti allineati a NengoDL (TF 2.12 + tf-keras 2.15)\\n!pip -q install -U pip setuptools wheel jedi==0.18.2\\n!pip -q install -r requirements_colab_nengo.txt\\n\\nprint(\\"‚úÖ Installazione completata. I warning di dependency conflicts sono normali e non bloccano l\\u2019esecuzione.\\")\\nprint(\\"‚ÑπÔ∏è  Adesso riavvio il runtime per applicare le modifiche...\\")\\n\\nimport os\\nos.kill(os.getpid(), 9)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Verifica versioni (dopo riavvio runtime)\\n!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\\n%cd /content/snn-ids\\n\\nimport tensorflow as tf\\n# Usa tf-keras invece di keras standalone per garantire keras.engine\\nimport tf_keras as keras\\nprint(\\"‚úÖ TensorFlow:\\", tf.__version__, \\"| tf-keras:\\", keras.__version__)\\n\\n# Test import NengoDL per verificare che keras.engine sia accessibile\\ntry:\\n    import nengo, nengo_dl\\n    print(\\"‚úÖ Nengo:\\", nengo.__version__, \\"| NengoDL:\\", nengo_dl.__version__)\\n    \\n    # Test specifico per keras.engine (quello che causava l\\u2019errore)\\n    import keras.engine\\n    print(\\"‚úÖ keras.engine trovato!\\")\\n    print(\\"‚úÖ Importazioni completate con successo!\\")\\nexcept Exception as e:\\n    print(\\"‚ùå Errore import:\\", e)\\n    print(\\"üîç Dettagli disponibili in tf.keras:\\")\\n    print(\\"   tf.keras moduli:\\", dir(tf.keras)[:10])  # Primi 10 per debug"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}
{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Download NSL-KDD (train/test) e conversione CSV‚Üídataset SNN via tool repo\\nfrom pathlib import Path\\nimport subprocess\\n\\nbase = Path(\\"/content/data\\")\\nbase.mkdir(parents=True, exist_ok=True)\\n\\n# Download dei file NSL-KDD\\nurls = {\\n    \\"KDDTrain+\\": \\"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt\\",\\n    \\"KDDTest+\\":  \\"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt\\",\\n}\\n\\nprint(\\"üì• Download NSL-KDD dataset...\\")\\nfor name, url in urls.items():\\n    !wget -q -O /content/data/{name}.txt \\"{url}\\"\\n\\nprint(\\"üîÑ Conversione in formato SNN...\\")\\n# Usa il tool della repo per convertire i CSV in formato SNN\\n!python -m tools.csv_to_snn_dataset --input /content/data/KDDTrain+.txt --output /content/data/nslkdd_snn_train.csv --label-col label\\n!python -m tools.csv_to_snn_dataset --input /content/data/KDDTest+.txt --output /content/data/nslkdd_snn_test.csv --label-col label\\n\\nprint(\\"üìÇ File creati:\\")\\n!ls -lh /content/data | head -n 10"]}
{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Carica dataset SNN + label reali dai raw\\nimport numpy as np, pandas as pd\\n\\nprint(\\"üìä Caricamento dataset SNN preprocessati...\\")\\ntrain_df = pd.read_csv(\\"/content/data/nslkdd_snn_train.csv\\")\\ntest_df  = pd.read_csv(\\"/content/data/nslkdd_snn_test.csv\\")\\n\\n# Label dai file raw originali per maggiore accuratezza\\ncols = [\\n  \\"duration\\",\\"protocol_type\\",\\"service\\",\\"flag\\",\\"src_bytes\\",\\"dst_bytes\\",\\"land\\",\\"wrong_fragment\\",\\"urgent\\",\\n  \\"hot\\",\\"num_failed_logins\\",\\"logged_in\\",\\"num_compromised\\",\\"root_shell\\",\\"su_attempted\\",\\"num_root\\",\\n  \\"num_file_creations\\",\\"num_shells\\",\\"num_access_files\\",\\"num_outbound_cmds\\",\\"is_host_login\\",\\"is_guest_login\\",\\n  \\"count\\",\\"srv_count\\",\\"serror_rate\\",\\"srv_serror_rate\\",\\"rerror_rate\\",\\"srv_rerror_rate\\",\\"same_srv_rate\\",\\n  \\"diff_srv_rate\\",\\"srv_diff_host_rate\\",\\"dst_host_count\\",\\"dst_host_srv_count\\",\\"dst_host_same_srv_rate\\",\\n  \\"dst_host_diff_srv_rate\\",\\"dst_host_same_src_port_rate\\",\\"dst_host_srv_diff_host_rate\\",\\"dst_host_serror_rate\\",\\n  \\"dst_host_srv_serror_rate\\",\\"dst_host_rerror_rate\\",\\"dst_host_srv_rerror_rate\\",\\"label\\",\\"difficulty\\"\\n]\\nraw_train = pd.read_csv(\\"/content/data/KDDTrain+.txt\\", names=cols)\\nraw_test  = pd.read_csv(\\"/content/data/KDDTest+.txt\\",  names=cols)\\ny_train = (raw_train[\\"label\\"].astype(str).str.lower() != \\"normal\\").astype(int).values\\ny_test  = (raw_test[\\"label\\"].astype(str).str.lower()  != \\"normal\\").astype(int).values\\n\\n# Estrai features dal dataset SNN (escludendo timestamp)\\nfeat_cols = [c for c in train_df.columns if c != \\"timestamp\\"]\\nX_train = train_df[feat_cols].values.astype(np.float32)\\nX_test  = test_df[feat_cols].values.astype(np.float32)\\n\\ninput_size, output_size = X_train.shape[1], 2\\nprint(f\\"üìà Shape dataset: Train {X_train.shape}, Test {X_test.shape}\\")\\nprint(f\\"üß† Configurazione rete: {input_size} input ‚Üí {output_size} output (binary classification)\\")\\nprint(f\\"üéØ Distribuzione label: Train {np.bincount(y_train)}, Test {np.bincount(y_test)}\\""]}
{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Costruzione rete Nengo e training con NengoDL\\nimport nengo, nengo_dl\\nimport numpy as np\\n\\nprint(\\"üîß Costruzione rete Nengo...\\")\\n\\n# Parametri della rete\\ntimesteps = 10\\nn_hidden = 256\\n\\n# Costruzione della rete SNN con Nengo\\nwith nengo.Network() as net:\\n    # Input layer (features)\\n    inp = nengo.Node(np.zeros(input_size))\\n    \\n    # Hidden layer con neuroni LIF\\n    ens1 = nengo.Ensemble(n_neurons=n_hidden, dimensions=input_size, neuron_type=nengo.LIF())\\n    nengo.Connection(inp, ens1)\\n    \\n    # Output layer (classificazione binaria)\\n    ens2 = nengo.Ensemble(n_neurons=output_size, dimensions=output_size, neuron_type=nengo.LIF())\\n    nengo.Connection(ens1, ens2)\\n    \\n    # Probe per raccogliere l\\u2019output\\n    p_out = nengo.Probe(ens2, synapse=0.1)\\n\\nprint(\\"üîÑ Encoding per rate coding (ripetizione features su timesteps)...\\")\\n# Rate coding: ripeti le features per T timesteps\\nX_train_enc = np.repeat(X_train[:, None, :], timesteps, axis=1)\\nX_test_enc  = np.repeat(X_test[:,  None, :], timesteps, axis=1)\\n\\n# One-hot encoding delle label per MSE loss\\ny_train_oh = keras.utils.to_categorical(y_train, num_classes=output_size)\\ny_test_oh  = keras.utils.to_categorical(y_test,  num_classes=output_size)\\ny_train_enc = np.repeat(y_train_oh[:, None, :], timesteps, axis=1)\\ny_test_enc  = np.repeat(y_test_oh[:,  None, :], timesteps, axis=1)\\n\\nprint(f\\"üìä Shape encoded: X_train {X_train_enc.shape}, y_train {y_train_enc.shape}\\")\\n\\nprint(\\"üöÄ Avvio training con NengoDL...\\")\\nminibatch = 64  # Ridotto per evitare OOM su Colab\\nepochs = 3      # Ridotto per demo rapida\\n\\nwith nengo_dl.Simulator(net, minibatch_size=minibatch) as sim:\\n    # Configurazione training\\n    inputs = {inp: X_train_enc}\\n    targets = {p_out: y_train_enc}\\n    \\n    sim.compile(optimizer=keras.optimizers.Adam(1e-3), loss=keras.losses.MSE)\\n    \\n    # Training con validation split\\n    sim.fit(inputs, targets, epochs=epochs, validation_split=0.2, verbose=2)\\n    \\n    print(\\"üß™ Valutazione su test set...\\")\\n    # Inferenza sui dati di test\\n    test_dict = {inp: X_test_enc}\\n    sim.run_steps(X_test_enc.shape[1], data=test_dict)\\n    out = sim.data[p_out]\\n\\n# Post-processing predizioni\\npred = out.mean(axis=1)  # Media su timesteps\\ny_pred = pred.argmax(axis=1)\\nacc = (y_pred == y_test).mean()\\n\\nprint(f\\"‚úÖ Test accuracy: {acc:.4f}\\")\\nprint(f\\"üéØ Predizioni: Normal {np.sum(y_pred == 0)}, Anomaly {np.sum(y_pred == 1)}\\""]}
{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Salvataggio artefatti del modello\\nimport os, pickle\\nfrom sklearn.metrics import classification_report, confusion_matrix\\n\\nprint(\\"üìä Report di classificazione dettagliato:\\")\\nprint(classification_report(y_test, y_pred, target_names=[\\"Normal\\", \\"Anomaly\\"]))\\n\\nprint(\\"üî¢ Confusion Matrix:\\")\\ncm = confusion_matrix(y_test, y_pred)\\nprint(f\\"   Predicted:  Normal  Anomaly\\")\\nprint(f\\"   Actual Normal:   {cm[0,0]:4d}    {cm[0,1]:4d}\\")\\nprint(f\\"        Anomaly:   {cm[1,0]:4d}    {cm[1,1]:4d}\\")\\n\\n# Salva la rete e le metriche\\nprint(\\"üíæ Salvataggio artefatti...\\")\\nos.makedirs(\\"/content/out\\", exist_ok=True)\\n\\n# Salva la rete Nengo\\nwith open(\\"/content/out/nengo_nslkdd_network.pkl\\", \\"wb\\") as f:\\n    pickle.dump(net, f)\\n\\n# Salva i risultati e metriche\\nresults = {\\n    \\"accuracy\\": acc,\\n    \\"predictions\\": y_pred,\\n    \\"true_labels\\": y_test,\\n    \\"confusion_matrix\\": cm,\\n    \\"network_config\\": {\\n        \\"input_size\\": input_size,\\n        \\"hidden_neurons\\": n_hidden,\\n        \\"output_size\\": output_size,\\n        \\"timesteps\\": timesteps,\\n        \\"epochs\\": epochs,\\n        \\"batch_size\\": minibatch\\n    }\\n}\\n\\nwith open(\\"/content/out/nengo_nslkdd_results.pkl\\", \\"wb\\") as f:\\n    pickle.dump(results, f)\\n\\nprint(\\"‚úÖ File salvati in /content/out/:\\")\\nprint(\\"   - nengo_nslkdd_network.pkl (rete Nengo)\\")\\nprint(\\"   - nengo_nslkdd_results.pkl (risultati e metriche)\\")\\n!ls -lh /content/out/"]}
]}
