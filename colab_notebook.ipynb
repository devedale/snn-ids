{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Ambiente di Lavoro\n",
    "\n",
    "Prima di tutto, è **necessario** configurare l'ambiente di lavoro. La cella seguente clonerà il repository GitHub contenente tutti i moduli Python (`.py`) e si posizionerà nella directory corretta. \n",
    "\n",
    "**Esegui questa cella come primo passo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clona il repository e posizionati nella directory corretta\n",
    "# NOTA: Assicurati che l'URL del repository e il nome del branch siano corretti.\n",
    "!git clone https://github.com/devedale/snn-ids.git snn-ids\n",
    "%cd snn-ids\n",
    "# Assicurati di usare il nome del branch corretto.\n",
    "!git checkout feat/modular-ml-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manoscritto della Pipeline ML per Cybersecurity\n",
    "\n",
    "Questo notebook è una guida interattiva e un manoscritto completo per una pipeline di machine learning **avanzata** per l'analisi di traffico di rete. È stato progettato con tre principi chiave in mente:\n",
    "\n",
    "- **Modularità**: Ogni fase logica (preprocessing, training, predizione) è isolata nel proprio modulo Python (`.py`).\n",
    "- **Configurabilità**: Tutta la pipeline è controllata da un singolo file, `config.py`.\n",
    "- **Estensibilità**: La struttura è pensata per essere facilmente adattabile a nuovi dataset o a nuove architetture di modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architettura e Scelte Tecniche\n",
    "\n",
    "La pipeline è composta dai seguenti elementi:\n",
    "\n",
    "- `create_synthetic_data.py`: Uno script per generare dati di esempio realistici.\n",
    "- `config.py`: Il cuore della configurazione. Qui si definiscono i percorsi, le colonne da usare e gli iperparametri per il training.\n",
    "- `preprocessing/process.py`: Contiene tutta la logica per caricare i dati e trasformarli in un formato numerico che il modello può comprendere.\n",
    "- `training/train.py`: Gestisce la creazione del modello, la grid search e il salvataggio del modello più performante.\n",
    "- `prediction/predict.py`: Carica un modello salvato e lo usa per fare predizioni su nuovi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Iniziale\n",
    "\n",
    "Installiamo le librerie necessarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa le dipendenze\n",
    "!pip install pandas scikit-learn tensorflow faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Guida Approfondita al File di Configurazione (`config.py`)\n",
    "\n",
    "Questa sezione è la più importante per personalizzare la pipeline. Vediamo ogni parametro in dettaglio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DATA_CONFIG`\n",
    "Controlla quali dati vengono usati.\n",
    "- `dataset_path`: Il percorso del tuo file CSV.\n",
    "- `timestamp_column`: Il nome esatto della colonna che contiene il timestamp. Essenziale per le finestre temporali.\n",
    "- `feature_columns`: La lista delle colonne che vuoi usare come feature iniziali. La pipeline si occuperà di trasformarle (es. one-hot encoding).\n",
    "- `ip_columns_to_anonymize`: Lista delle colonne che contengono indirizzi IP. Verranno trasformate in ID numerici.\n",
    "- `target_column`: Il nome della colonna che contiene l'etichetta da predire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PREPROCESSING_CONFIG`\n",
    "Controlla come i dati vengono trasformati in sequenze.\n",
    "- `use_time_windows`: Se `True`, attiva la logica di finestre temporali. Se `False`, ogni riga è un campione a sé. Impostalo a `False` se vuoi usare un modello non sequenziale come `'dense'`.\n",
    "- `window_size`: **Parametro cruciale**. Definisce quanti eventi (righe) compongono una singola sequenza. Una finestra più grande cattura pattern a lungo termine, ma richiede più memoria e dati. Una finestra piccola cattura pattern a breve termine. La scelta dipende dal tipo di attacco che vuoi rilevare.\n",
    "- `step`: Di quanti eventi si sposta la finestra. Se `step = 1`, generi il massimo numero di finestre sovrapposte. Se `step = window_size`, le finestre non si sovrappongono. Un valore intermedio (es. `step = window_size / 2`) è un buon compromesso tra quantità di dati generati e ridondanza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TRAINING_CONFIG`\n",
    "Il cuore del training.\n",
    "- `validation_strategy`: Come detto, `'k_fold'` è più robusto, `'train_test_split'` è più veloce.\n",
    "- `model_type`: Scegli l'architettura. `'lstm'` è fatta per le sequenze create da `use_time_windows=True`.\n",
    "- `hyperparameters`: **Qui avviene la magia della multi-permutazione**. Per ogni parametro, puoi inserire una lista di valori da testare. Lo script di training creerà un modello per *ogni singola combinazione possibile*.\n",
    "  - **Esempio di Test Multi-Permutazione**: Se imposti:\n",
    "    ```python\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"learning_rate\": [0.001, 0.01],\n",
    "    ```\n",
    "    La grid search testerà 4 combinazioni: (16, 0.001), (16, 0.01), (32, 0.001), (32, 0.01).\n",
    "  - **Consigli per il Tuning**:\n",
    "    - `batch_size`: Parti con 32 o 64. Se il tuo dataset è enorme, prova valori più alti. Se il modello non converge bene, prova valori più bassi.\n",
    "    - `epochs`: Parti con un numero medio (es. 20-50). Se vedi che il modello migliora ancora alla fine, aumenta le epoche.\n",
    "    - `learning_rate`: È il parametro più sensibile. Un range comune da testare è `[0.01, 0.001, 0.0001]`.\n",
    "    - `lstm_units`: Controlla la capacità del modello LSTM. Parti con valori come 32, 64 o 128. Aumenta se sospetti che il modello sia troppo semplice (underfitting), diminuisci se va in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Esecuzione della Pipeline Completa\n",
    "Ora che abbiamo capito la configurazione, eseguiamo l'intera pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- FASE 1: Generazione Dati ---\")\n",
    "from create_synthetic_data import generate_synthetic_data\n",
    "generate_synthetic_data()\n",
    "\n",
    "print(\"\\n--- FASE 2: Preprocessing in Finestre Temporali ---\")\n",
    "from preprocessing.process import preprocess_data\n",
    "X, y = preprocess_data()\n",
    "if X is not None:\n",
    "    print(f\"\\nShape di X: {X.shape}\")\n",
    "    print(f\"Shape di y: {y.shape}\")\n",
    "\n",
    "print(\"\\n--- FASE 3: Training del Modello ---\")\n",
    "from training.train import train_and_evaluate\n",
    "_, best_model_path = train_and_evaluate()\n",
    "\n",
    "print(\"\\n--- FASE 4: Predizione su Nuovi Dati ---\")\n",
    "from prediction.predict import predict_on_window\n",
    "from config import PREPROCESSING_CONFIG\n",
    "window_size = PREPROCESSING_CONFIG.get('window_size', 10)\n",
    "sample_window = [\n",
    "    {\n",
    "        \"ip_sorgente\": \"192.168.1.10\", \"ip_destinazione\": \"10.0.0.5\",\n",
    "        \"porta_sorgente\": 12345, \"porta_destinazione\": 443,\n",
    "        \"protocollo\": \"UDP\", \"byte_inviati\": 250, \"byte_ricevuti\": 1800\n",
    "    } for _ in range(window_size)\n",
    "]\n",
    "prediction_label = predict_on_window(sample_window)\n",
    "if prediction_label:\n",
    "    print(f\"\\nRISULTATO FINALE: La finestra di dati è stata classificata come: '{prediction_label}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Guida Passo-Passo: Adattare la Pipeline al Tuo Dataset\n",
    "\n",
    "Questa guida ti mostra come usare i tuoi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passaggio 1: Aggiorna `config.py`\n",
    "\n",
    "Immagina di avere un nuovo dataset `my_traffic.csv` con le seguenti colonne: `['Timestamp', 'SourceIP', 'DestIP', 'SourcePort', 'DestPort', 'ProtocolType', 'PacketCount', 'TrafficType']`.\n",
    "\n",
    "Per adattare la pipeline, dovrai modificare `DATA_CONFIG` in `config.py` così:\n",
    "\n",
    "```python\n",
    "DATA_CONFIG = {\n",
    "    \"dataset_path\": \"data/my_traffic.csv\",\n",
    "    \"timestamp_column\": \"Timestamp\",\n",
    "    \"feature_columns\": [\"SourcePort\", \"DestPort\", \"ProtocolType\", \"PacketCount\"],\n",
    "    \"ip_columns_to_anonymize\": [\"SourceIP\", \"DestIP\"],\n",
    "    \"target_column\": \"TrafficType\",\n",
    "}\n",
    "```\n",
    "**Fatto!** La pipeline ora userà i tuoi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passaggio 2: Creare Nuove Feature (Feature Engineering)\n",
    "\n",
    "Spesso, le feature migliori non sono quelle originali, ma quelle che creiamo noi. Immaginiamo di voler creare una feature `is_well_known_port` che è `1` se la porta di destinazione è una porta comune (es. 80, 443) e `0` altrimenti.\n",
    "\n",
    "1. Apri `preprocessing/process.py`.\n",
    "2. Trova la sezione dove viene caricato il DataFrame `df`, subito dopo `pd.read_csv`.\n",
    "3. **Aggiungi il tuo codice** per creare la nuova colonna:\n",
    "\n",
    "```python\n",
    "# Esempio da aggiungere in process.py\n",
    "well_known_ports = [80, 443, 22, 21, 53]\n",
    "df['is_well_known_port'] = df[DATA_CONFIG['feature_columns'][1]].isin(well_known_ports).astype(int)\n",
    "```\n",
    "\n",
    "4. Infine, **aggiorna `config.py`** per includere la tua nuova feature nella lista `feature_columns`:\n",
    "\n",
    "```python\n",
    "\"feature_columns\": [\"SourcePort\", \"DestPort\", \"ProtocolType\", \"PacketCount\", \"is_well_known_port\"],\n",
    "```\n",
    "Questo approccio ti dà la flessibilità di creare logiche di preprocessing complesse mantenendo la configurazione semplice e centralizzata."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
