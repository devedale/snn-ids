{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN-IDS: Pipeline di Intrusion Detection con Federated Learning e Homomorphic Encryption\n",
    "\n",
    "Questo notebook è la guida principale e interattiva per il progetto SNN-IDS. L'obiettivo è fornire una spiegazione chiara e riproducibile delle scelte architetturali e tecniche, permettendo di rieseguire l'intero flusso di lavoro, dalla preparazione dei dati al training di modelli complessi come quelli federati e sicuri.\n",
    "\n",
    "**Obiettivi del Notebook:**\n",
    "1.  **Trasparenza:** Documentare ogni fase del processo.\n",
    "2.  **Riproducibilità:** Consentire di rieseguire gli esperimenti e ottenere risultati coerenti.\n",
    "3.  **Modularità:** Interagire con il codice sorgente, strutturato come una libreria (`snn_ids`), senza duplicare la logica.\n",
    "\n",
    "**Struttura del Notebook:**\n",
    "*   **Parte 1: Setup e Configurazione:** Preparazione dell'ambiente e analisi del file di configurazione centrale.\n",
    "*   **Parte 2: La Pipeline dei Dati:** Caricamento, preprocessing e caching.\n",
    "*   **Parte 3: Esperimento - Training Centralizzato (Baseline):** Addestramento di un modello standard per avere un riferimento.\n",
    "*   **Parte 4: Esperimento - Federated Learning (FL):** Simulazione di un addestramento federato.\n",
    "*   **Parte 5: Esperimento - Secure FL con Homomorphic Encryption (HE):** Aggiunta di un livello di privacy con la crittografia omomorfica.\n",
    "*   **Parte 6: Valutazione e Confronto:** Analisi dei risultati dei vari esperimenti.\n",
    "*   **Parte 7: Benchmarking Automatizzato:** Come usare lo script `benchmark.py` per test su larga scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Setup e Configurazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Installazione delle Dipendenze\n",
    "\n",
    "Assicuriamoci che tutte le librerie necessarie siano installate. Il file `requirements.txt` contiene la lista completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import delle Librerie e Moduli del Progetto\n",
    "\n",
    "Importiamo le librerie standard e i moduli principali dal nostro package `snn_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Aggiungiamo la directory 'src' al path per permettere l'import del nostro package\n",
    "sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "from config import DATA_CONFIG, PREPROCESSING_CONFIG, TRAINING_CONFIG, FEDERATED_CONFIG, HOMOMORPHIC_CONFIG\n",
    "from snn_ids.preprocessing.process import preprocess_pipeline\n",
    "\n",
    "print(\"Librerie e moduli caricati con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Analisi della Configurazione Centrale (`config.py`)\n",
    "\n",
    "Tutto il comportamento della pipeline è controllato da un unico file, `config.py`. Questo approccio centralizzato (Single Source of Truth) evita di avere parametri \"magici\" sparsi nel codice e rende gli esperimenti facili da configurare e riprodurre.\n",
    "\n",
    "Esaminiamo le sezioni principali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- CONFIGURAZIONE DATI ---\")\n",
    "print(json.dumps(DATA_CONFIG, indent=2))\n",
    "\n",
    "print(\"\\n--- CONFIGURAZIONE PREPROCESSING ---\")\n",
    "print(json.dumps(PREPROCESSING_CONFIG, indent=2))\n",
    "\n",
    "print(\"\\n--- CONFIGURAZIONE FEDERATED LEARNING ---\")\n",
    "print(json.dumps(FEDERATED_CONFIG, indent=2))\n",
    "\n",
    "print(\"\\n--- CONFIGURAZIONE HOMOMORPHIC ENCRYPTION ---\")\n",
    "print(json.dumps(HOMOMORPHIC_CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: La Pipeline dei Dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Caching dei Dati\n",
    "\n",
    "Il preprocessing dei dati è un'operazione costosa. Per ottimizzare il processo, il nostro sistema utilizza due livelli di cache:\n",
    "\n",
    "1.  **`preprocessed_cache`**: Contiene i file CSV originali già parzialmente puliti e con le classi bilanciate. Questo evita di dover rileggere e filtrare i file sorgente ad ogni esecuzione.\n",
    "2.  **`model_cache`**: Contiene i dati finali pronti per il training (array NumPy `X` e `y`, e il `LabelEncoder`). Questo è il livello di cache più veloce e viene usato per accelerare esperimenti ripetuti con la stessa configurazione di dati.\n",
    "\n",
    "Per questo notebook, useremo la `model_cache` che è già presente nel repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Esecuzione del Preprocessing\n",
    "\n",
    "Eseguiamo la pipeline di preprocessing. Questa funzione caricherà i dati direttamente dalla `model_cache` se la trova."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, label_encoder = preprocess_pipeline(\n",
    "    sample_size=10000 # Questo valore corrisponde a quello della model_cache\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing completato.\")\n",
    "print(f\"Forma di X: {X.shape}\")\n",
    "print(f\"Forma di y: {y.shape}\")\n",
    "print(f\"Numero di classi: {len(label_encoder.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Esecuzione tramite Runner Unificato\n",
    "\n",
    "Tutta la logica per l'esecuzione dei diversi tipi di esperimenti è stata consolidata nel runner `benchmark.py`. Questo script permette di lanciare workflow complessi con un singolo comando.\n",
    "\n",
    "Qui sotto mostriamo come eseguire i principali workflow direttamente dal notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Esperimento Centralizzato (Smoke Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 benchmark.py centralized --smoke-test --sample-size 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Esperimento Federato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 benchmark.py federated --sample-size 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Esperimento Federato con Homomorphic Encryption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 benchmark.py federated --he --sample-size 1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
