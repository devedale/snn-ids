{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Guided Tour of a Deep Learning Pipeline for Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Welcome to this guided tour of a modern, end-to-end machine learning pipeline for network intrusion detection. This notebook is designed to be a comprehensive and didactic resource, explaining not just *how* to build the pipeline, but *why* we make certain design choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. The Challenge: Network Intrusion Detection\n",
    "\n",
    "In today's interconnected world, network security is paramount. An Intrusion Detection System (IDS) is like a digital security guard, monitoring network traffic for malicious activity or policy violations. The challenge is that the volume and complexity of network traffic are immense, and attackers are constantly evolving their methods. Traditional, rule-based systems struggle to keep up. This is where machine learning, and specifically deep learning, can offer a powerful solution by learning to recognize the complex patterns of both normal and malicious behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Our Approach: A Modular & Configurable Pipeline\n",
    "\n",
    "The goal of this project is to build a robust and understandable deep learning pipeline. To achieve this, we follow three key principles:\n",
    "\n",
    "- **Modularity**: Each logical phase of the project (e.g., data preprocessing, model training) is separated into its own Python script (`.py` file). This makes the code cleaner, easier to maintain, and reusable.\n",
    "- **Configurability**: The entire pipeline is controlled from a central configuration file (`config.py`). This allows us to easily experiment with different models, features, and hyperparameters without changing the core logic.\n",
    "- **Didacticism**: The notebook itself is designed to be a learning tool. We will explain each step, from theory to code, to make the entire process transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. The Dataset: CSE-CIC-IDS2018\n",
    "\n",
    "A model is only as good as its data. For this project, we will use the **CSE-CIC-IDS2018 dataset**. This is an excellent choice because it's a large, realistic dataset that includes a wide variety of modern network attacks, along with benign traffic. It was developed by the Canadian Institute for Cybersecurity.\n",
    "\n",
    "For further reading on the dataset and its creation, we refer to the original paper:\n",
    "*Sharafaldin, I., Lashkari, A. H., & Ghorbani, A. A. (2018). Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Concepts\n",
    "\n",
    "Before we dive into the code, let's briefly cover some of the core concepts that underpin our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. The Machine Learning Workflow\n",
    "\n",
    "A typical supervised machine learning project follows a standard workflow:\n",
    "\n",
    "1.  **Data Acquisition**: Get the data.\n",
    "2.  **Data Preprocessing**: Clean, transform, and feature-engineer the data into a numerical format suitable for the model.\n",
    "3.  **Model Training**: \"Fit\" a model to the preprocessed data. The model learns the relationship between the features and the target labels.\n",
    "4.  **Model Evaluation**: Assess the model's performance on unseen data to see how well it generalizes.\n",
    "5.  **Prediction/Inference**: Use the trained model to make predictions on new, real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Our Project's Architecture\n",
    "\n",
    "To implement this workflow in a clean and organized way, our project is structured into several key files:\n",
    "\n",
    "- `ids_pipeline_explained.ipynb`: This notebook. It acts as the main interface and guide.\n",
    "- `config.py`: The central configuration file. It holds all our settings, like file paths, feature lists, and model hyperparameters.\n",
    "- `preprocessing/process.py`: A Python module containing all the logic for data loading and preprocessing.\n",
    "- `training/train.py`: A module that handles model creation (e.g., LSTM, GRU), training, and evaluation.\n",
    "- `prediction/predict.py`: A module for using a saved model to make predictions (not used in this notebook, but part of the complete project)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Key Model Architectures\n",
    "\n",
    "We will experiment with a few different types of neural network layers, each with different strengths:\n",
    "\n",
    "- **Dense Layers**: The most basic building block of neural networks. Each neuron in a dense layer is connected to every neuron in the previous layer. They are great for learning general patterns but don't have any inherent understanding of sequence or time.\n",
    "- **Recurrent Neural Networks (RNNs)**: Designed to work with sequential data. They have a 'memory' that allows them to retain information from previous inputs in a sequence. We will use two advanced types of RNNs:\n",
    "    - **LSTM (Long Short-Term Memory)**: A sophisticated type of RNN that is very effective at learning long-range dependencies in a sequence. It uses a series of 'gates' to control what information to remember and what to forget.\n",
    "    - **GRU (Gated Recurrent Unit)**: A slightly simpler and more modern version of the LSTM. It also uses gates to manage memory but has fewer parameters, which can make it faster to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "Now, let's move from theory to practice. The following cells will set up our working environment. We only need to run these once at the beginning of our session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Cloning the Project Repository\n",
    "\n",
    "Our project is designed to be modular, with code organized into separate `.py` files. To access this code, we first need to clone the project's GitHub repository into our local environment. The following command will download the repository and move our current working directory inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This will clone the main branch. Ensure your desired code is on this branch.\n",
    "!git clone https://github.com/devedale/snn-ids.git snn-ids\n",
    "%cd snn-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Installing Dependencies\n",
    "\n",
    "Next, we need to install the Python libraries required for this project. We've listed them in a `requirements.txt` file for convenience, but for clarity in this notebook, we'll install them directly using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn tensorflow numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Importing Libraries & Reloading Modules\n",
    "\n",
    "With our environment set up, we can now import the necessary libraries and our own custom modules. \n",
    "\n",
    "**An Important Note on Notebooks**: Jupyter notebooks sometimes 'cache' imported modules. If you make a change to a `.py` file (like `train.py`), the notebook might not automatically pick up the change. To prevent this, we use the `importlib` library to explicitly `reload` our custom modules. This ensures we are always running the latest version of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add the project root to the Python path to allow for module imports\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Import our custom modules\n",
    "from preprocessing import process\n",
    "from training import train\n",
    "\n",
    "# Reload our modules to ensure the latest code is used\n",
    "importlib.reload(process)\n",
    "importlib.reload(train)\n",
    "\n",
    "# Now, we can safely import the functions we need from our reloaded modules\n",
    "from training.train import train_and_evaluate\n",
    "\n",
    "print(\"Libraries imported and custom modules reloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Acquisition and Preparation\n",
    "\n",
    "With our environment ready, it's time to get our hands on the data. This section covers downloading, inspecting, and preparing the dataset for our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Downloading the Dataset\n",
    "\n",
    "The following commands will download the CSE-CIC-IDS2018 dataset from Kaggle (using a direct link) and unzip it into a `data/` directory. This is a large file, so this step may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Downloads\n",
    "!curl -L -o ./Downloads/improved-cicids2017-and-csecicids2018.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/ernie55ernie/improved-cicids2017-and-csecicids2018\n",
    "\n",
    "!unzip -o ./Downloads/improved-cicids2017-and-csecicids2018.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Inspecting the Raw Data\n",
    "\n",
    "It's always a good idea to look at your raw data before you start processing it. Let's use the `head` command to peek at the first few lines of one of the CSV files. This helps us understand the format, see the column names, and get a feel for the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 6 ./data/CSECICIDS2018_improved/Friday-02-03-2018.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. The Preprocessing Pipeline Explained\n",
    "\n",
    "The raw CSV files are not suitable for a neural network. We need to convert the data into a purely numerical format. All the logic for this is contained in the `preprocess_data` function within our `preprocessing/process.py` module. Here's a summary of what it does:\n",
    "\n",
    "1.  **Load Data**: It reads one or more CSV files into a pandas DataFrame.\n",
    "2.  **Sample Data**: For quick tests, it can take a smaller, random sample of the data.\n",
    "3.  **Clean Data**: It drops rows with missing values and removes any infinite values that could cause errors during training.\n",
    "4.  **Feature Selection**: It selects only the specific feature columns defined in our `config.py` file. It also separates the features (X) from the target label (y).\n",
    "5.  **Encode Labels**: It converts the text-based labels (e.g., 'Benign', 'Bot') into integer representations (e.g., 0, 1).\n",
    "6.  **Scale Numerical Features**: It uses `StandardScaler` from scikit-learn to scale all numerical features. This is a critical step that standardizes the range of the features, which helps the model train more effectively.\n",
    "7.  **Reshape for Sequential Models**: For models like LSTMs and GRUs that expect sequences, it reshapes the data into a 3D tensor of `(samples, timesteps, features)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Executing the Preprocessing (Smoke Test)\n",
    "\n",
    "Let's run just the preprocessing step on a small sample of the data. This is often called a \"smoke test\"—a quick run to make sure everything is working correctly without waiting for a full training cycle. We'll define a configuration to only use 10,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a configuration for a quick smoke test\n",
    "smoke_test_config = {\n",
    "    \"PREPROCESSING_CONFIG\": {\n",
    "        \"sample_size\": 10000\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the preprocessing function\n",
    "X_processed, y_processed = process.preprocess_data(config_override=smoke_test_config)\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Inspecting the Processed Data\n",
    "\n",
    "Now, let's look at the output of our preprocessing. We should see that our data has been transformed from a table of mixed text and numbers into a numerical tensor, ready for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the processed features (X): {X_processed.shape}\")\n",
    "print(f\"Shape of the processed labels (y): {y_processed.shape}\")\n",
    "print(\"\\n--- Example of a single processed sample ---\")\n",
    "print(X_processed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "With our data preprocessed and ready, it's time for the most exciting part: training a neural network model. In this section, we'll configure our training run, execute it, and learn how to interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. The Experimentation Hub: Configure Your Model\n",
    "\n",
    "This code cell is the control panel for our training pipeline. By simply editing this Python dictionary, you can change fundamental aspects of the model and the training process. For our first run, we'll keep it simple: we'll use a GRU model and train for just a few epochs. \n",
    "\n",
    "Here are the key parameters you can change:\n",
    "- `model_type`: Choose between `'lstm'`, `'gru'`, or `'dense'`.\n",
    "- `validation_strategy`: We'll use `'train_test_split'` which is a standard way to evaluate a model. It holds back a portion of the data (the test set) to see how well the trained model generalizes to unseen data.\n",
    "- `epochs`: The number of times the model will see the entire training dataset. \n",
    "- `batch_size`: The number of samples the model looks at before updating its internal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXPERIMENTATION HUB ---\n",
    "# Modify this dictionary to configure your training run.\n",
    "experiment_config = {\n",
    "    \"TRAINING_CONFIG\": {\n",
    "        \"model_type\": \"gru\",\n",
    "        \"validation_strategy\": \"train_test_split\",\n",
    "        \"hyperparameters\": {\n",
    "            \"epochs\": [5],\n",
    "            \"batch_size\": [128],\n",
    "            \"learning_rate\": [0.001],\n",
    "            \"activation\": [\"relu\"],\n",
    "            \"gru_units\": [64],\n",
    "            \"lstm_units\": [64] # This is ignored when model_type is 'gru'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Experiment configuration loaded:\")\n",
    "print(json.dumps(experiment_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Running the Training\n",
    "\n",
    "Now, let's pass our preprocessed data (`X_processed`, `y_processed`) and our configuration to the `train_and_evaluate` function. This function, which lives in `training/train.py`, will:\n",
    "\n",
    "1.  Build the model specified in our configuration.\n",
    "2.  Split the data into training and testing sets.\n",
    "3.  Train the model on the training set, while evaluating its performance on the test set after each epoch.\n",
    "4.  Save the best-performing model to a file (`best_model.keras`).\n",
    "5.  Log the results to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Pass the preprocessed data directly to the training function\n",
    "training_log, best_model_path = train_and_evaluate(\n",
    "    X=X_processed, \n",
    "    y=y_processed, \n",
    "    config_override=experiment_config\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- TRAINING COMPLETE in {(end_time - start_time):.2f} seconds ---\")\n",
    "print(f\"Best model saved to: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Understanding the Results\n",
    "\n",
    "The output above shows the training process epoch by epoch. For each epoch, you see the `loss` and `accuracy` on the training data, and the `val_loss` and `val_accuracy` on the validation (test) set. \n",
    "\n",
    "- **Accuracy**: The percentage of predictions the model got right. A higher number is better.\n",
    "- **Loss**: A measure of how wrong the model's predictions are. A lower number is better.\n",
    "\n",
    "The key metric to watch is `val_accuracy`. This tells you how well your model is performing on data it has never seen before, which is the true test of its generalization ability."
   ]
  }
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have successfully walked through a complete, end-to-end deep learning pipeline for network intrusion detection. Let's recap what we've accomplished and discuss where you can go from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Summary of Achievements\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1.  **Set up a modular project structure**, cloning a repository with organized Python modules.\n",
    "2.  **Downloaded and inspected a real-world cybersecurity dataset**.\n",
    "3.  **Built and executed a preprocessing pipeline** to transform raw data into model-ready numerical tensors.\n",
    "4.  **Configured and trained a deep learning model** (a GRU network) using a flexible 'Experimentation Hub'.\n",
    "5.  **Evaluated the model's performance** and learned how to interpret the key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. How to Experiment Further\n",
    "\n",
    "The true power of this pipeline is its flexibility. The best way to learn is to experiment. We encourage you to go back and change the configurations to see how they affect the model's performance. Here are a few ideas:\n",
    "\n",
    "-   **Try a Different Model**: Go back to the **Experimentation Hub** in Section 5.1 and change `model_type` from `'gru'` to `'lstm'` or `'dense'`. Do they perform better or worse? Are they faster or slower to train?\n",
    "-   **Train for Longer**: Increase the number of `epochs` in the configuration. Does the model's accuracy continue to improve, or does it start to overfit?\n",
    "-   **Run on More Data**: Go back to **Section 4.4** where we defined the `smoke_test_config`. Change the `sample_size` to a larger number (e.g., `100000`) or even `None` to use the entire dataset. This will take much longer to run, but should yield a much more powerful model.\n",
    "-   **Tune Hyperparameters**: Adjust the `learning_rate` or the number of `gru_units` in the configuration. These small changes can have a big impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Future Directions\n",
    "\n",
    "This pipeline provides a solid foundation that can be extended in many exciting ways. Some advanced topics you could explore next include:\n",
    "\n",
    "-   **Advanced Anonymization**: Researching and implementing techniques like Crypto-PAn to anonymize sensitive data like IP addresses while preserving network structure.\n",
    "-   **Spiking Neural Networks (SNNs)**: Exploring a new generation of more biologically plausible and potentially more energy-efficient neural networks.\n",
    "-   **Detailed Performance Analysis**: Going beyond accuracy to look at metrics like Precision, Recall, and F1-Score for each attack category.\n",
    "\n",
    "Thank you for following along, and happy experimenting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
