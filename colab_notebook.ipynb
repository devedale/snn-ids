{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Ambiente di Lavoro\n",
    "\n",
    "Prima di tutto, è **necessario** configurare l'ambiente di lavoro. La cella seguente clonerà il repository GitHub contenente tutti i moduli Python (`.py`) e si posizionerà nella directory corretta. \n",
    "\n",
    "**Esegui questa cella come primo passo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clona il repository e posizionati nella directory corretta\n",
    "# NOTA: Assicurati che l'URL del repository e il nome del branch siano corretti.\n",
    "!git clone https://github.com/devedale/snn-ids.git snn-ids\n",
    "%cd snn-ids\n",
    "# Assicurati di usare il nome del branch corretto.\n",
    "!git checkout feat/modular-ml-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manoscritto della Pipeline ML per Cybersecurity\n",
    "\n",
    "Questo notebook è una guida interattiva e un manoscritto completo per una pipeline di machine learning **avanzata** per l'analisi di traffico di rete. È stato progettato con tre principi chiave in mente:\n",
    "\n",
    "- **Modularità**: Ogni fase logica (preprocessing, training, predizione) è isolata nel proprio modulo Python (`.py`).\n",
    "- **Configurabilità**: Tutta la pipeline è controllata da un singolo file, `config.py`.\n",
    "- **Estensibilità**: La struttura è pensata per essere facilmente adattabile a nuovi dataset o a nuove architetture di modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architettura e Scelte Tecniche\n",
    "\n",
    "La pipeline è composta dai seguenti elementi:\n",
    "\n",
    "- `config.py`: Il cuore della configurazione. Qui si definiscono i percorsi, le colonne da usare e gli iperparametri per il training.\n",
    "- `preprocessing/process.py`: Contiene tutta la logica per caricare i dati e trasformarli in un formato numerico che il modello può comprendere.\n",
    "- `training/train.py`: Gestisce la creazione del modello, la grid search e il salvataggio del modello più performante.\n",
    "- `prediction/predict.py`: Carica un modello salvato e lo usa per fare predizioni su nuovi dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Iniziale\n",
    "\n",
    "Installiamo le librerie necessarie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installa le dipendenze\n",
    "!pip install pandas scikit-learn tensorflow faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download ed Estrazione del Dataset\n",
    "\n",
    "La cella seguente si occupa di scaricare il dataset zippato da Kaggle e di estrarlo nella directory `data/`. Questo passaggio è fondamentale per avere i dati pronti per l'analisi.\n",
    "\n",
    "**Nota**: Questo comando utilizza `curl` e `unzip`, che sono standard in ambienti Linux come Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p Downloads\n",
    "!curl -L -o ./Downloads/improved-cicids2017-and-csecicids2018.zip \\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/ernie55ernie/improved-cicids2017-and-csecicids2018\n",
    "\n",
    "!unzip -o ./Downloads/improved-cicids2017-and-csecicids2018.zip -d ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica dell'Estrazione\n",
    "\n",
    "Eseguiamo un rapido controllo per assicurarci che i file siano stati estratti correttamente, visualizzando le prime righe di uno dei file CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 6 ./data/CSECICIDS2018_improved/Friday-02-03-2018.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adattamento della Configurazione\n",
    "\n",
    "Il file `config.py` è il cuore della nostra pipeline. Lo abbiamo già pre-configurato per funzionare con il nuovo dataset. Questa sezione spiega le modifiche chiave che sono state fatte. Puoi modificare `config.py` per sperimentare con diverse feature, iperparametri o strategie.\n",
    "\n",
    "In particolare, puoi cambiare il `model_type` in `\"gru\"` per utilizzare la nuova architettura di rete neurale che abbiamo aggiunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Smoke Test: Esecuzione Rapida su un Campione\n",
    "\n",
    "Questa sezione esegue l'intera pipeline su un piccolo sottoinsieme dei dati (le prime 10.000 righe). È un passaggio cruciale per verificare rapidamente che tutte le componenti (preprocessing, training, predizione) funzionino correttamente con la configurazione attuale, senza attendere i tempi di un training completo.\n",
    "\n",
    "**Obiettivo**: Confermare la funzionalità end-to-end, non ottenere un modello performante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from preprocessing.process import preprocess_data\n",
    "from training.train import train_and_evaluate\n",
    "from prediction.predict import predict_on_window\n",
    "from config import PREPROCESSING_CONFIG\n",
    "\n",
    "SMOKE_TEST_SAMPLE_SIZE = 10000\n",
    "\n",
    "print(f\"--- SMOKE TEST: Inizio dell'esecuzione su {SMOKE_TEST_SAMPLE_SIZE} campioni ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Fase 1: Preprocessing su un campione\n",
    "print(\"\\n--- FASE 1: Preprocessing ---\")\n",
    "X_sample, y_sample = preprocess_data(sample_size=SMOKE_TEST_SAMPLE_SIZE)\n",
    "\n",
    "# Fase 2: Training su un campione\n",
    "best_model_path_sample = None\n",
    "if X_sample is not None and y_sample is not None:\n",
    "    print(\"\\n--- FASE 2: Training ---\")\n",
    "    _, best_model_path_sample = train_and_evaluate(X_sample, y_sample)\n",
    "else:\n",
    "    print(\"Dati di esempio non generati, salto il training.\")\n",
    "\n",
    "# Fase 3: Predizione di Esempio\n",
    "print(\"\\n--- FASE 3: Predizione ---\")\n",
    "if best_model_path_sample:\n",
    "    window_size = PREPROCESSING_CONFIG.get('window_size', 10)\n",
    "    # Creiamo una finestra di dati fittizi per la predizione\n",
    "    sample_window = [\n",
    "        {\n",
    "            'Src IP': '10.0.0.1', 'Dst IP': '10.0.0.2', 'Src Port': 12345, 'Dst Port': 80, 'Protocol': 6,\n",
    "            'Flow Duration': 1000, 'Total Fwd Packet': 5, 'Total Bwd packets': 3, 'Total Length of Fwd Packet': 200,\n",
    "            'Total Length of Bwd Packet': 300, 'Flow Bytes/s': 500000, 'Flow Packets/s': 8000, 'Flow IAT Mean': 125.0,\n",
    "            'Flow IAT Std': 50.0, 'Flow IAT Max': 200, 'Flow IAT Min': 100, 'Fwd IAT Mean': 250.0, 'Bwd IAT Mean': 333.0,\n",
    "            'Fwd Header Length': 100, 'Bwd Header Length': 60, 'Average Packet Size': 62.5, 'Fwd Segment Size Avg': 40.0,\n",
    "            'Bwd Segment Size Avg': 100.0\n",
    "        } for _ in range(window_size)\n",
    "    ]\n",
    "    prediction_label = predict_on_window(sample_window, model_path=best_model_path_sample)\n",
    "    if prediction_label:\n",
    "        print(f\"\\nRISULTATO DELLO SMOKE TEST: La finestra di dati è stata classificata come: '{prediction_label}'\")\n",
    "else:\n",
    "    print(\"Nessun modello addestrato nello smoke test, impossibile fare una predizione.\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n--- SMOKE TEST COMPLETATO in {end_time - start_time:.2f} secondi ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Esecuzione Completa: Training sul Dataset Intero\n",
    "\n",
    "Questa è la sezione principale per addestrare il modello sull'intero dataset. A differenza dello smoke test, questa esecuzione utilizzerà tutti i dati disponibili per produrre un modello il più performante possibile.\n",
    "\n",
    "**Attenzione**: L'esecuzione di questa cella richiederà una notevole quantità di tempo e risorse (RAM e CPU/GPU). Si consiglia di eseguirla in un ambiente con GPU attivata (come Google Colab Pro) e di essere pronti a un'attesa significativa.\n",
    "\n",
    "Puoi modificare gli iperparametri nel file `config.py` per bilanciare tempo di training e performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from preprocessing.process import preprocess_data\n",
    "from training.train import train_and_evaluate\n",
    "\n",
    "print(\"--- ESECUZIONE COMPLETA: Inizio del training sul dataset intero ---\")\n",
    "start_time_full = time.time()\n",
    "\n",
    "# Fase 1: Preprocessing completo\n",
    "print(\"\\n--- FASE 1: Preprocessing (Dataset Completo) ---\")\n",
    "# Chiamiamo preprocess_data() senza sample_size per usare tutti i dati\n",
    "X_full, y_full = preprocess_data()\n",
    "\n",
    "# Fase 2: Training completo\n",
    "if X_full is not None and y_full is not None:\n",
    "    print(\"\\n--- FASE 2: Training (Dataset Completo) ---\")\n",
    "    train_and_evaluate(X_full, y_full)\n",
    "else:\n",
    "    print(\"Errore nel preprocessing dei dati completi, training annullato.\")\n",
    "\n",
    "end_time_full = time.time()\n",
    "print(f\"\\n--- ESECUZIONE COMPLETA TERMINATA in {(end_time_full - start_time_full) / 60:.2f} minuti ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusioni e Prossimi Passi\n",
    "\n",
    "Questo notebook ha fornito una struttura completa e modulare per affrontare un problema di classificazione del traffico di rete utilizzando un dataset reale e complesso. Abbiamo visto come:\n",
    "\n",
    "1.  **Impostare l'ambiente** e scaricare un dataset di grandi dimensioni.\n",
    "2.  **Adattare la configurazione** della pipeline (`config.py`) per lavorare con le nuove feature.\n",
    "3.  **Eseguire uno \"Smoke Test\"** per verificare rapidamente la funzionalità end-to-end.\n",
    "4.  **Lanciare un training completo** per costruire un modello performante.\n",
    "\n",
    "### Prossimi Passi\n",
    "\n",
    "Da qui, ci sono molte direzioni interessanti da esplorare:\n",
    "\n",
    "-   **Sperimentazione con le Feature**: Prova a modificare la lista `feature_columns` in `config.py`. Aggiungere o rimuovere feature può avere un impatto significativo sulle performance.\n",
    "-   **Ottimizzazione degli Iperparametri**: Esegui una Grid Search più ampia modificando il dizionario `hyperparameters` in `config.py`. Prova diversi tassi di apprendimento, dimensioni dei batch o architetture di rete.\n",
    "-   **Analisi degli Errori**: Esamina i casi in cui il modello sbaglia le previsioni. Questo può rivelare pattern specifici che il modello fatica a riconoscere e suggerire nuove feature da creare.\n",
    "-   **Ricerca Accademica**: Come discusso, il prossimo passo potrebbe essere quello di cercare pubblicazioni che usano questo dataset e tentare di replicare le loro metodologie per confrontare i risultati.\n",
    "\n",
    "Buon lavoro!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
