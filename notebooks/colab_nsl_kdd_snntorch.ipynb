{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN su NSL-KDD (Colab) — baseline con snntorch\n",
        "\n",
        "Questo notebook scarica NSL-KDD, esegue preprocessing (numeriche normalizzate + categoriche one-hot), applica rate coding e allena una SNN minimale con snntorch. Funziona su Google Colab (CPU/GPU).\n",
        "\n",
        "- Dataset: NSL-KDD (KDDTrain+ / KDDTest+)\n",
        "- Modello: MLP con neuroni Leaky IF (snntorch)\n",
        "- Codifica: Rate encoding con T=10 step\n",
        "- Obiettivo: binaria (anomalo vs normale)\n",
        "\n",
        "Suggerimenti:\n",
        "- Runtime → Cambia tipo di runtime → GPU (facoltativo)\n",
        "- Epoche e dimensioni layer possono essere aumentate su GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Installazione dipendenze\n",
        "!pip -q install snntorch pyyaml pandas scikit-learn\n",
        "\n",
        "import os, sys, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NSL-KDD (train/test)\n",
        "base = Path('/content/data'); base.mkdir(parents=True, exist_ok=True)\n",
        "urls = {\n",
        "    'KDDTrain+': 'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt',\n",
        "    'KDDTest+':  'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt',\n",
        "}\n",
        "for name, url in urls.items():\n",
        "    !wget -q -O /content/data/{name}.txt \"{url}\"\n",
        "!ls -lh /content/data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing: split, one-hot, normalizzazione\n",
        "cols = [\n",
        "  \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\n",
        "  \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "  \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\n",
        "  \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "  \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
        "  \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
        "  \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"\n",
        "]\n",
        "\n",
        "def load_split(split):\n",
        "  df = pd.read_csv(f\"/content/data/{split}.txt\", names=cols)\n",
        "  df.drop(columns=[\"difficulty\"], inplace=True, errors=\"ignore\")\n",
        "  return df\n",
        "\n",
        "train_df, test_df = load_split(\"KDDTrain+\"), load_split(\"KDDTest+\")\n",
        "\n",
        "y_train = (train_df[\"label\"] != \"normal\").astype(int).values\n",
        "y_test  = (test_df[\"label\"]  != \"normal\").astype(int).values\n",
        "X_train_df = train_df.drop(columns=[\"label\"])\n",
        "X_test_df  = test_df.drop(columns=[\"label\"])\n",
        "\n",
        "cat_cols = [\"protocol_type\",\"service\",\"flag\"]\n",
        "num_cols = [c for c in X_train_df.columns if c not in cat_cols]\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train_num = scaler.fit_transform(X_train_df[num_cols].values.astype(float))\n",
        "X_test_num  = scaler.transform(X_test_df[num_cols].values.astype(float))\n",
        "\n",
        "X_train_cat = pd.get_dummies(X_train_df[cat_cols].astype(str)).values\n",
        "X_test_cat  = pd.get_dummies(X_test_df[cat_cols].astype(str)).reindex(\n",
        "    columns=pd.get_dummies(X_train_df[cat_cols].astype(str)).columns, fill_value=0\n",
        ").values\n",
        "\n",
        "X_train = np.hstack([X_train_num, X_train_cat]).astype(np.float32)\n",
        "X_test  = np.hstack([X_test_num,  X_test_cat]).astype(np.float32)\n",
        "input_size, output_size = X_train.shape[1], 2\n",
        "X_train.shape, X_test.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rate coding e SNN con snntorch\n",
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "timesteps = 10\n",
        "\n",
        "def rate_encode(X, timesteps=10):\n",
        "  Xc = torch.tensor(X, device=device)\n",
        "  return Xc.unsqueeze(1).repeat(1, timesteps, 1)\n",
        "\n",
        "train_data = TensorDataset(rate_encode(X_train, timesteps), torch.tensor(y_train, device=device))\n",
        "test_data  = TensorDataset(rate_encode(X_test, timesteps),   torch.tensor(y_test,  device=device))\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=512, shuffle=False)\n",
        "\n",
        "class SNNNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden=256, output_size=2):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden)\n",
        "    self.lif1 = snn.Leaky(beta=0.9)\n",
        "    self.fc2 = nn.Linear(hidden, output_size)\n",
        "    self.lif2 = snn.Leaky(beta=0.9)\n",
        "  def forward(self, x):\n",
        "    mem1 = self.lif1.init_leaky(); mem2 = self.lif2.init_leaky()\n",
        "    spk_sum = 0\n",
        "    for t in range(x.size(1)):\n",
        "      cur = self.fc1(x[:, t, :])\n",
        "      spk1, mem1 = self.lif1(cur, mem1)\n",
        "      cur2 = self.fc2(spk1)\n",
        "      spk2, mem2 = self.lif2(cur2, mem2)\n",
        "      spk_sum = spk_sum + spk2\n",
        "    return spk_sum\n",
        "\n",
        "model = SNNNet(input_size, 256, output_size).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training e valutazione\n",
        "import numpy as np\n",
        "\n",
        "def accuracy(logits, y):\n",
        "  preds = torch.argmax(logits, dim=1)\n",
        "  return (preds == y).float().mean().item()\n",
        "\n",
        "for epoch in range(5):\n",
        "  model.train(); losses=[]; accs=[]\n",
        "  for Xb, yb in train_loader:\n",
        "    opt.zero_grad()\n",
        "    logits = model(Xb)\n",
        "    loss = criterion(logits, yb)\n",
        "    loss.backward(); opt.step()\n",
        "    losses.append(loss.item()); accs.append(accuracy(logits, yb))\n",
        "  print(f\"Epoch {epoch+1}: loss={np.mean(losses):.4f}, acc={np.mean(accs):.3f}\")\n",
        "\n",
        "model.eval(); accs=[]\n",
        "with torch.no_grad():\n",
        "  for Xb, yb in test_loader:\n",
        "    logits = model(Xb)\n",
        "    accs.append(accuracy(logits, yb))\n",
        "print(f\"Test Accuracy: {np.mean(accs):.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvataggio artefatti\n",
        "Path('/content/out').mkdir(exist_ok=True)\n",
        "torch.save(model.state_dict(), \"/content/out/snn_nslkdd.pt\")\n",
        "np.save(\"/content/out/scaler_min.npy\", scaler.min_)\n",
        "np.save(\"/content/out/scaler_scale.npy\", scaler.scale_)\n",
        "!ls -lh /content/out\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
