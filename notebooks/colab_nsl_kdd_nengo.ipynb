{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SNN (Nengo/NengoDL) su NSL-KDD ‚Äî Colab\n",
        "\n",
        "Notebook end-to-end: clona la repo, installa requisiti, scarica NSL-KDD, preprocessing, rete Nengo con neuroni LIF, training con NengoDL, valutazione e salvataggio parametri.\n",
        "\n",
        "- Repo: https://github.com/devedale/snn-ids\n",
        "- Framework: Nengo + NengoDL (TensorFlow backend)\n",
        "- Codifica: rate coding (ripetizione su T step)\n",
        "- Task: binaria (anomalo vs normale)\n",
        "\n",
        "**Nota sui conflitti pip**: I warning di dependency conflicts sono normali in Colab. NengoDL richiede TensorFlow 2.13.1 + numpy 1.24.3 che confliggono con alcuni pacchetti preinstallati, ma il notebook funziona comunque.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup ambiente + install compatibili (NengoDL) e riavvio runtime\n",
        "!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\n",
        "%cd /content/snn-ids\n",
        "\n",
        "# Rimuove pacchetti preinstallati che confliggono con TF/Keras 2.x\n",
        "!pip -q uninstall -y jax jaxlib tensorflow-text tensorflow-decision-forests tensorflow-hub \\\n",
        "  orbax-checkpoint chex optax flax dopamine-rl tensorstore || true\n",
        "!pip -q uninstall -y tensorflow tensorflow-cpu tensorflow-gpu keras-hub opencv-python opencv-contrib-python opencv-python-headless || true\n",
        "!pip -q uninstall -y keras keras-nightly tf-keras keras-preprocessing keras-vis || true\n",
        "!pip -q uninstall -y albucore albumentations ydf grpcio-status typeguard torch torchvision torchaudio fastai sentence-transformers peft timm accelerate || true\n",
        "\n",
        "# Aggiorna toolchain e installa requisiti allineati a NengoDL (TF/Keras 2.13.x)\n",
        "!pip -q install -U pip setuptools wheel jedi==0.18.2\n",
        "!pip -q install -r requirements_colab_nengo.txt\n",
        "\n",
        "print(\"‚úÖ Installazione completata. I warning di dependency conflicts sono normali e non bloccano l'esecuzione.\")\n",
        "print(\"‚ÑπÔ∏è  Adesso riavvio il runtime per applicare le modifiche...\")\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verifica versioni (dopo riavvio runtime)\n",
        "!rm -rf /content/snn-ids && git clone https://github.com/devedale/snn-ids.git /content/snn-ids -q\n",
        "%cd /content/snn-ids\n",
        "\n",
        "import tensorflow as tf, keras\n",
        "print(\"‚úÖ TensorFlow:\", tf.__version__, \"| Keras:\", keras.__version__)\n",
        "\n",
        "# Test import NengoDL per verificare che keras.engine sia accessibile\n",
        "try:\n",
        "    import nengo, nengo_dl\n",
        "    print(\"‚úÖ Nengo:\", nengo.__version__, \"| NengoDL:\", nengo_dl.__version__)\n",
        "    print(\"‚úÖ Importazioni completate con successo!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Errore import NengoDL:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NSL-KDD (train/test) e conversione CSV‚Üídataset SNN via tool repo\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "base = Path('/content/data')\n",
        "base.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download dei file NSL-KDD\n",
        "urls = {\n",
        "    'KDDTrain+': 'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain+.txt',\n",
        "    'KDDTest+':  'https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTest+.txt',\n",
        "}\n",
        "\n",
        "print(\"üì• Download NSL-KDD dataset...\")\n",
        "for name, url in urls.items():\n",
        "    !wget -q -O /content/data/{name}.txt \"{url}\"\n",
        "\n",
        "print(\"üîÑ Conversione in formato SNN...\")\n",
        "# Usa il tool della repo per convertire i CSV in formato SNN\n",
        "!python -m tools.csv_to_snn_dataset --input /content/data/KDDTrain+.txt --output /content/data/nslkdd_snn_train.csv --label-col label\n",
        "!python -m tools.csv_to_snn_dataset --input /content/data/KDDTest+.txt --output /content/data/nslkdd_snn_test.csv --label-col label\n",
        "\n",
        "print(\"üìÇ File creati:\")\n",
        "!ls -lh /content/data | head -n 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carica dataset SNN + label reali dai raw\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "print(\"üìä Caricamento dataset SNN preprocessati...\")\n",
        "train_df = pd.read_csv('/content/data/nslkdd_snn_train.csv')\n",
        "test_df  = pd.read_csv('/content/data/nslkdd_snn_test.csv')\n",
        "\n",
        "# Label dai file raw originali per maggiore accuratezza\n",
        "cols = [\n",
        "  \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\n",
        "  \"hot\",\"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "  \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\n",
        "  \"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "  \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\n",
        "  \"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
        "  \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"\n",
        "]\n",
        "\n",
        "print(\"üè∑Ô∏è  Estrazione label originali...\")\n",
        "raw_train = pd.read_csv('/content/data/KDDTrain+.txt', names=cols)\n",
        "raw_test  = pd.read_csv('/content/data/KDDTest+.txt',  names=cols)\n",
        "y_train = (raw_train['label'].astype(str).str.lower() != 'normal').astype(int).values\n",
        "y_test  = (raw_test['label'].astype(str).str.lower()  != 'normal').astype(int).values\n",
        "\n",
        "# Estrai features dal dataset SNN (escludendo timestamp)\n",
        "feat_cols = [c for c in train_df.columns if c != 'timestamp']\n",
        "X_train = train_df[feat_cols].values.astype(np.float32)\n",
        "X_test  = test_df[feat_cols].values.astype(np.float32)\n",
        "\n",
        "input_size, output_size = X_train.shape[1], 2\n",
        "print(f\"üìà Shape dataset: Train {X_train.shape}, Test {X_test.shape}\")\n",
        "print(f\"üß† Configurazione rete: {input_size} input ‚Üí {output_size} output (binary classification)\")\n",
        "print(f\"üéØ Distribuzione label: Train {np.bincount(y_train)}, Test {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Costruzione rete Nengo e training con NengoDL\n",
        "import nengo, nengo_dl, tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(\"üîß Costruzione rete Nengo...\")\n",
        "\n",
        "# Parametri della rete\n",
        "timesteps = 10\n",
        "n_hidden = 256\n",
        "\n",
        "# Costruzione della rete SNN con Nengo\n",
        "with nengo.Network() as net:\n",
        "    # Input layer (features)\n",
        "    inp = nengo.Node(np.zeros(input_size))\n",
        "    \n",
        "    # Hidden layer con neuroni LIF\n",
        "    ens1 = nengo.Ensemble(n_neurons=n_hidden, dimensions=input_size, neuron_type=nengo.LIF())\n",
        "    nengo.Connection(inp, ens1)\n",
        "    \n",
        "    # Output layer (classificazione binaria)\n",
        "    ens2 = nengo.Ensemble(n_neurons=output_size, dimensions=output_size, neuron_type=nengo.LIF())\n",
        "    nengo.Connection(ens1, ens2)\n",
        "    \n",
        "    # Probe per raccogliere l'output\n",
        "    p_out = nengo.Probe(ens2, synapse=0.1)\n",
        "\n",
        "print(\"üîÑ Encoding per rate coding (ripetizione features su timesteps)...\")\n",
        "# Rate coding: ripeti le features per T timesteps\n",
        "X_train_enc = np.repeat(X_train[:, None, :], timesteps, axis=1)\n",
        "X_test_enc  = np.repeat(X_test[:,  None, :], timesteps, axis=1)\n",
        "\n",
        "# One-hot encoding delle label per MSE loss\n",
        "y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes=output_size)\n",
        "y_test_oh  = tf.keras.utils.to_categorical(y_test,  num_classes=output_size)\n",
        "y_train_enc = np.repeat(y_train_oh[:, None, :], timesteps, axis=1)\n",
        "y_test_enc  = np.repeat(y_test_oh[:,  None, :], timesteps, axis=1)\n",
        "\n",
        "print(f\"üìä Shape encoded: X_train {X_train_enc.shape}, y_train {y_train_enc.shape}\")\n",
        "\n",
        "print(\"üöÄ Avvio training con NengoDL...\")\n",
        "minibatch = 64  # Ridotto per evitare OOM su Colab\n",
        "epochs = 3      # Ridotto per demo rapida\n",
        "\n",
        "with nengo_dl.Simulator(net, minibatch_size=minibatch) as sim:\n",
        "    # Configurazione training\n",
        "    inputs = {inp: X_train_enc}\n",
        "    targets = {p_out: y_train_enc}\n",
        "    \n",
        "    sim.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=tf.keras.losses.MSE)\n",
        "    \n",
        "    # Training con validation split\n",
        "    sim.fit(inputs, targets, epochs=epochs, validation_split=0.2, verbose=2)\n",
        "    \n",
        "    print(\"üß™ Valutazione su test set...\")\n",
        "    # Inferenza sui dati di test\n",
        "    test_dict = {inp: X_test_enc}\n",
        "    sim.run_steps(X_test_enc.shape[1], data=test_dict)\n",
        "    out = sim.data[p_out]\n",
        "\n",
        "# Post-processing predizioni\n",
        "pred = out.mean(axis=1)  # Media su timesteps\n",
        "y_pred = pred.argmax(axis=1)\n",
        "acc = (y_pred == y_test).mean()\n",
        "\n",
        "print(f\"‚úÖ Test accuracy: {acc:.4f}\")\n",
        "print(f\"üéØ Predizioni: Normal {np.sum(y_pred == 0)}, Anomaly {np.sum(y_pred == 1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvataggio artefatti del modello\n",
        "import os, pickle\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"üìä Report di classificazione dettagliato:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Normal', 'Anomaly']))\n",
        "\n",
        "print(\"üî¢ Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"   Predicted:  Normal  Anomaly\")\n",
        "print(f\"Actual Normal:   {cm[0,0]:4d}    {cm[0,1]:4d}\")\n",
        "print(f\"     Anomaly:   {cm[1,0]:4d}    {cm[1,1]:4d}\")\n",
        "\n",
        "# Salva la rete e le metriche\n",
        "print(\"üíæ Salvataggio artefatti...\")\n",
        "os.makedirs('/content/out', exist_ok=True)\n",
        "\n",
        "# Salva la rete Nengo\n",
        "with open('/content/out/nengo_nslkdd_network.pkl', 'wb') as f:\n",
        "    pickle.dump(net, f)\n",
        "\n",
        "# Salva i risultati e metriche\n",
        "results = {\n",
        "    'accuracy': acc,\n",
        "    'predictions': y_pred,\n",
        "    'true_labels': y_test,\n",
        "    'confusion_matrix': cm,\n",
        "    'network_config': {\n",
        "        'input_size': input_size,\n",
        "        'hidden_neurons': n_hidden,\n",
        "        'output_size': output_size,\n",
        "        'timesteps': timesteps,\n",
        "        'epochs': epochs,\n",
        "        'batch_size': minibatch\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('/content/out/nengo_nslkdd_results.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)\n",
        "\n",
        "print(\"‚úÖ File salvati in /content/out/:\")\n",
        "print(\"   - nengo_nslkdd_network.pkl (rete Nengo)\")\n",
        "print(\"   - nengo_nslkdd_results.pkl (risultati e metriche)\")\n",
        "!ls -lh /content/out/\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
